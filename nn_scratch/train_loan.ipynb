{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df08c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from nn import NeuralNetwork\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4973d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_metrics(y_true, y_pred, multiclass=False):\n",
    "\n",
    "    average_type = 'macro' if multiclass else 'binary'\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "\n",
    "    print(f\"✅ Accuracy : {acc:.4f}\")\n",
    "    print(f\"✅ Precision: {prec:.4f}\")\n",
    "    print(f\"✅ Recall   : {rec:.4f}\")\n",
    "    print(f\"✅ F1 Score : {f1:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a40fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>issue_d_month</th>\n",
       "      <th>issue_d_year</th>\n",
       "      <th>earliest_cr_line_month</th>\n",
       "      <th>earliest_cr_line_year</th>\n",
       "      <th>last_pymnt_d_month</th>\n",
       "      <th>last_pymnt_d_year</th>\n",
       "      <th>last_credit_pull_d_month</th>\n",
       "      <th>last_credit_pull_d_year</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24341</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.21</td>\n",
       "      <td>387.17</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67534</td>\n",
       "      <td>33850.0</td>\n",
       "      <td>33850.0</td>\n",
       "      <td>33775.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>20.99</td>\n",
       "      <td>915.57</td>\n",
       "      <td>1 year</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>1984</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35080</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>20.00</td>\n",
       "      <td>264.94</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>36580.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4828</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.31</td>\n",
       "      <td>695.15</td>\n",
       "      <td>9 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>48700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>1996</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59259</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.99</td>\n",
       "      <td>866.52</td>\n",
       "      <td>1 year</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  loan_amnt  funded_amnt  funded_amnt_inv        term  int_rate  \\\n",
       "0  24341    12500.0      12500.0          12500.0   36 months      7.21   \n",
       "1  67534    33850.0      33850.0          33775.0   60 months     20.99   \n",
       "2  35080    10000.0      10000.0          10000.0   60 months     20.00   \n",
       "3   4828    20250.0      20250.0          20250.0   36 months     14.31   \n",
       "4  59259    25000.0      25000.0          25000.0   36 months     14.99   \n",
       "\n",
       "   installment emp_length home_ownership  annual_inc  ...  \\\n",
       "0       387.17   < 1 year       MORTGAGE     81000.0  ...   \n",
       "1       915.57     1 year       MORTGAGE     80000.0  ...   \n",
       "2       264.94   < 1 year           RENT     36580.0  ...   \n",
       "3       695.15    9 years           RENT     48700.0  ...   \n",
       "4       866.52     1 year       MORTGAGE     85000.0  ...   \n",
       "\n",
       "  debt_settlement_flag issue_d_month issue_d_year earliest_cr_line_month  \\\n",
       "0                    N             6         2018                      6   \n",
       "1                    N            10         2015                      9   \n",
       "2                    N             9         2017                     10   \n",
       "3                    N             0         2015                      6   \n",
       "4                    N            11         2016                      0   \n",
       "\n",
       "  earliest_cr_line_year  last_pymnt_d_month  last_pymnt_d_year  \\\n",
       "0                  2000                   2               2019   \n",
       "1                  1984                   2               2019   \n",
       "2                  2006                   1               2018   \n",
       "3                  1996                   6               2016   \n",
       "4                  2002                   2               2019   \n",
       "\n",
       "   last_credit_pull_d_month  last_credit_pull_d_year  grade  \n",
       "0                         2                     2019      A  \n",
       "1                         2                     2019      E  \n",
       "2                        11                     2018      D  \n",
       "3                         9                     2017      C  \n",
       "4                         2                     2019      C  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dat\n",
    "train_data = pd.read_csv(\n",
    "    'data/loan-10k.lrn.csv'\n",
    ")\n",
    "\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850bc479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Separate features and target\n",
    "X = train_data.drop(columns=[\"ID\", \"grade\"])\n",
    "y = train_data[\"grade\"]\n",
    "\n",
    "# 2. Label encode target (A, B, C → 0, 1, 2)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# 3. One-hot encode target for softmax\n",
    "ohe_y = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "y_onehot = ohe_y.fit_transform(y_encoded.reshape(-1, 1))\n",
    "\n",
    "# 4. Identify feature types\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "# 5. Preprocess features: One-hot categorical + scale numeric\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    (\"num\", StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# 6. Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y_onehot, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82211cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    # Simple\n",
    "    {\"layers\": [X_train.shape[1],y_train.shape[1]], \"act\": [utils.softmax], \"lr\": 0.01},\n",
    "    {\"layers\": [X_train.shape[1], y_train.shape[1]], \"act\": [utils.softmax], \"lr\": 0.1},\n",
    "\n",
    "    # 1 hidden layer\n",
    "    {\"layers\": [X_train.shape[1], 16, y_train.shape[1]], \"act\": [utils.relu, utils.softmax], \"lr\": 0.01},\n",
    "    {\"layers\": [X_train.shape[1], 16, y_train.shape[1]], \"act\": [utils.tanh, utils.softmax], \"lr\": 0.01},\n",
    "    {\"layers\": [X_train.shape[1], 16, y_train.shape[1]], \"act\": [utils.relu, utils.softmax], \"lr\": 0.1},\n",
    "    {\"layers\": [X_train.shape[1], 16, y_train.shape[1]], \"act\": [utils.tanh, utils.softmax], \"lr\": 0.1},\n",
    "\n",
    "    # 2 hidden layers\n",
    "    {\"layers\": [X_train.shape[1], 16, 8, y_train.shape[1]], \"act\": [utils.relu, utils.relu, utils.softmax], \"lr\": 0.01},\n",
    "    {\"layers\": [X_train.shape[1], 16, 8, y_train.shape[1]], \"act\": [utils.tanh, utils.relu, utils.softmax], \"lr\": 0.01},\n",
    "    {\"layers\": [X_train.shape[1], 16, 8, y_train.shape[1]], \"act\": [utils.tanh, utils.tanh, utils.softmax], \"lr\": 0.01},\n",
    "    {\"layers\": [X_train.shape[1], 16, 8, y_train.shape[1]], \"act\": [utils.relu, utils.relu, utils.softmax], \"lr\": 0.1},\n",
    "    {\"layers\": [X_train.shape[1], 16, 8, y_train.shape[1]], \"act\": [utils.tanh, utils.relu, utils.softmax], \"lr\": 0.1},\n",
    "    {\"layers\": [X_train.shape[1], 16, 8, y_train.shape[1]], \"act\": [utils.tanh, utils.tanh, utils.softmax], \"lr\":0.1}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139fca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Running config 1/12\n",
      "Layers: [179, 7], Activations: ['softmax'], LR: 0.01\n",
      "Epoch 1/100, Loss: 2.7666, Acc: 0.4308, Val Loss: 1.4462, Val Acc: 0.5410\n",
      "Epoch 2/100, Loss: 1.0855, Acc: 0.6081, Val Loss: 1.0561, Val Acc: 0.6335\n",
      "Epoch 3/100, Loss: 0.8257, Acc: 0.6870, Val Loss: 0.8887, Val Acc: 0.6785\n",
      "Epoch 4/100, Loss: 0.6934, Acc: 0.7298, Val Loss: 0.8009, Val Acc: 0.7080\n",
      "Epoch 5/100, Loss: 0.6202, Acc: 0.7575, Val Loss: 0.7354, Val Acc: 0.7325\n",
      "Epoch 6/100, Loss: 0.5695, Acc: 0.7796, Val Loss: 0.6975, Val Acc: 0.7445\n",
      "Epoch 7/100, Loss: 0.5309, Acc: 0.7930, Val Loss: 0.6588, Val Acc: 0.7510\n",
      "Epoch 8/100, Loss: 0.5068, Acc: 0.8063, Val Loss: 0.6386, Val Acc: 0.7590\n",
      "Epoch 9/100, Loss: 0.4840, Acc: 0.8145, Val Loss: 0.6153, Val Acc: 0.7705\n",
      "Epoch 10/100, Loss: 0.4602, Acc: 0.8210, Val Loss: 0.5974, Val Acc: 0.7795\n",
      "Epoch 11/100, Loss: 0.4516, Acc: 0.8275, Val Loss: 0.5822, Val Acc: 0.7810\n",
      "Epoch 12/100, Loss: 0.4328, Acc: 0.8327, Val Loss: 0.5714, Val Acc: 0.7885\n",
      "Epoch 13/100, Loss: 0.4280, Acc: 0.8367, Val Loss: 0.5578, Val Acc: 0.7910\n",
      "Epoch 14/100, Loss: 0.4116, Acc: 0.8414, Val Loss: 0.5494, Val Acc: 0.7975\n",
      "Epoch 15/100, Loss: 0.4098, Acc: 0.8454, Val Loss: 0.5390, Val Acc: 0.7975\n",
      "Epoch 16/100, Loss: 0.3949, Acc: 0.8475, Val Loss: 0.5321, Val Acc: 0.8020\n",
      "Epoch 17/100, Loss: 0.3946, Acc: 0.8494, Val Loss: 0.5234, Val Acc: 0.8020\n",
      "Epoch 18/100, Loss: 0.3810, Acc: 0.8548, Val Loss: 0.5185, Val Acc: 0.8075\n",
      "Epoch 19/100, Loss: 0.3823, Acc: 0.8554, Val Loss: 0.5111, Val Acc: 0.8075\n",
      "Epoch 20/100, Loss: 0.3698, Acc: 0.8581, Val Loss: 0.5078, Val Acc: 0.8095\n",
      "Epoch 21/100, Loss: 0.3718, Acc: 0.8578, Val Loss: 0.5016, Val Acc: 0.8095\n",
      "Epoch 22/100, Loss: 0.3612, Acc: 0.8610, Val Loss: 0.4985, Val Acc: 0.8135\n",
      "Epoch 23/100, Loss: 0.3639, Acc: 0.8611, Val Loss: 0.4928, Val Acc: 0.8155\n",
      "Epoch 24/100, Loss: 0.3523, Acc: 0.8636, Val Loss: 0.4911, Val Acc: 0.8150\n",
      "Epoch 25/100, Loss: 0.3550, Acc: 0.8641, Val Loss: 0.4864, Val Acc: 0.8180\n",
      "Epoch 26/100, Loss: 0.3452, Acc: 0.8655, Val Loss: 0.4840, Val Acc: 0.8175\n",
      "Epoch 27/100, Loss: 0.3499, Acc: 0.8668, Val Loss: 0.4796, Val Acc: 0.8230\n",
      "Epoch 28/100, Loss: 0.3383, Acc: 0.8679, Val Loss: 0.4789, Val Acc: 0.8180\n",
      "Epoch 29/100, Loss: 0.3422, Acc: 0.8682, Val Loss: 0.4755, Val Acc: 0.8220\n",
      "Epoch 30/100, Loss: 0.3328, Acc: 0.8699, Val Loss: 0.4734, Val Acc: 0.8205\n",
      "Epoch 31/100, Loss: 0.3386, Acc: 0.8701, Val Loss: 0.4698, Val Acc: 0.8230\n",
      "Epoch 32/100, Loss: 0.3268, Acc: 0.8718, Val Loss: 0.4701, Val Acc: 0.8215\n",
      "Epoch 33/100, Loss: 0.3313, Acc: 0.8711, Val Loss: 0.4670, Val Acc: 0.8225\n",
      "Epoch 34/100, Loss: 0.3235, Acc: 0.8728, Val Loss: 0.4663, Val Acc: 0.8245\n",
      "Epoch 35/100, Loss: 0.3292, Acc: 0.8719, Val Loss: 0.4625, Val Acc: 0.8260\n",
      "Epoch 36/100, Loss: 0.3182, Acc: 0.8732, Val Loss: 0.4632, Val Acc: 0.8260\n",
      "Epoch 37/100, Loss: 0.3235, Acc: 0.8735, Val Loss: 0.4608, Val Acc: 0.8265\n",
      "Epoch 38/100, Loss: 0.3151, Acc: 0.8744, Val Loss: 0.4597, Val Acc: 0.8275\n",
      "Epoch 39/100, Loss: 0.3222, Acc: 0.8746, Val Loss: 0.4569, Val Acc: 0.8290\n",
      "Epoch 40/100, Loss: 0.3110, Acc: 0.8752, Val Loss: 0.4577, Val Acc: 0.8285\n",
      "Epoch 41/100, Loss: 0.3167, Acc: 0.8755, Val Loss: 0.4553, Val Acc: 0.8275\n",
      "Epoch 42/100, Loss: 0.3080, Acc: 0.8751, Val Loss: 0.4543, Val Acc: 0.8295\n",
      "Epoch 43/100, Loss: 0.3160, Acc: 0.8774, Val Loss: 0.4518, Val Acc: 0.8285\n",
      "Epoch 44/100, Loss: 0.3047, Acc: 0.8778, Val Loss: 0.4525, Val Acc: 0.8280\n",
      "Epoch 45/100, Loss: 0.3111, Acc: 0.8780, Val Loss: 0.4503, Val Acc: 0.8290\n",
      "Epoch 46/100, Loss: 0.3024, Acc: 0.8791, Val Loss: 0.4490, Val Acc: 0.8305\n",
      "Epoch 47/100, Loss: 0.3106, Acc: 0.8798, Val Loss: 0.4478, Val Acc: 0.8310\n",
      "Epoch 48/100, Loss: 0.3005, Acc: 0.8799, Val Loss: 0.4482, Val Acc: 0.8290\n",
      "Epoch 49/100, Loss: 0.3057, Acc: 0.8802, Val Loss: 0.4462, Val Acc: 0.8305\n",
      "Epoch 50/100, Loss: 0.2987, Acc: 0.8809, Val Loss: 0.4458, Val Acc: 0.8295\n",
      "Epoch 51/100, Loss: 0.3051, Acc: 0.8820, Val Loss: 0.4445, Val Acc: 0.8300\n",
      "Epoch 52/100, Loss: 0.2957, Acc: 0.8825, Val Loss: 0.4444, Val Acc: 0.8305\n",
      "Epoch 53/100, Loss: 0.3003, Acc: 0.8820, Val Loss: 0.4436, Val Acc: 0.8320\n",
      "Epoch 54/100, Loss: 0.2942, Acc: 0.8824, Val Loss: 0.4425, Val Acc: 0.8305\n",
      "Epoch 55/100, Loss: 0.3001, Acc: 0.8832, Val Loss: 0.4413, Val Acc: 0.8325\n",
      "Epoch 56/100, Loss: 0.2920, Acc: 0.8839, Val Loss: 0.4416, Val Acc: 0.8320\n",
      "Epoch 57/100, Loss: 0.2955, Acc: 0.8838, Val Loss: 0.4387, Val Acc: 0.8325\n",
      "Epoch 58/100, Loss: 0.2901, Acc: 0.8848, Val Loss: 0.4404, Val Acc: 0.8330\n",
      "Epoch 59/100, Loss: 0.2954, Acc: 0.8841, Val Loss: 0.4381, Val Acc: 0.8340\n",
      "Epoch 60/100, Loss: 0.2889, Acc: 0.8850, Val Loss: 0.4394, Val Acc: 0.8340\n",
      "Epoch 61/100, Loss: 0.2941, Acc: 0.8851, Val Loss: 0.4382, Val Acc: 0.8355\n",
      "Epoch 62/100, Loss: 0.2873, Acc: 0.8842, Val Loss: 0.4377, Val Acc: 0.8360\n",
      "Epoch 63/100, Loss: 0.2944, Acc: 0.8858, Val Loss: 0.4366, Val Acc: 0.8360\n",
      "Epoch 64/100, Loss: 0.2854, Acc: 0.8860, Val Loss: 0.4371, Val Acc: 0.8345\n",
      "Epoch 65/100, Loss: 0.2895, Acc: 0.8854, Val Loss: 0.4338, Val Acc: 0.8350\n",
      "Epoch 66/100, Loss: 0.2871, Acc: 0.8866, Val Loss: 0.4381, Val Acc: 0.8345\n",
      "Epoch 67/100, Loss: 0.2863, Acc: 0.8859, Val Loss: 0.4326, Val Acc: 0.8360\n",
      "Epoch 68/100, Loss: 0.2858, Acc: 0.8872, Val Loss: 0.4377, Val Acc: 0.8350\n",
      "Epoch 69/100, Loss: 0.2853, Acc: 0.8862, Val Loss: 0.4316, Val Acc: 0.8385\n",
      "Epoch 70/100, Loss: 0.2860, Acc: 0.8869, Val Loss: 0.4368, Val Acc: 0.8360\n",
      "Epoch 71/100, Loss: 0.2838, Acc: 0.8866, Val Loss: 0.4310, Val Acc: 0.8385\n",
      "Epoch 72/100, Loss: 0.2833, Acc: 0.8876, Val Loss: 0.4363, Val Acc: 0.8370\n",
      "Epoch 73/100, Loss: 0.2828, Acc: 0.8866, Val Loss: 0.4301, Val Acc: 0.8395\n",
      "Epoch 74/100, Loss: 0.2836, Acc: 0.8878, Val Loss: 0.4354, Val Acc: 0.8400\n",
      "Epoch 75/100, Loss: 0.2813, Acc: 0.8872, Val Loss: 0.4296, Val Acc: 0.8405\n",
      "Epoch 76/100, Loss: 0.2809, Acc: 0.8889, Val Loss: 0.4339, Val Acc: 0.8390\n",
      "Epoch 77/100, Loss: 0.2800, Acc: 0.8884, Val Loss: 0.4290, Val Acc: 0.8410\n",
      "Epoch 78/100, Loss: 0.2764, Acc: 0.8889, Val Loss: 0.4319, Val Acc: 0.8400\n",
      "Epoch 79/100, Loss: 0.2813, Acc: 0.8891, Val Loss: 0.4289, Val Acc: 0.8410\n",
      "Epoch 80/100, Loss: 0.2754, Acc: 0.8895, Val Loss: 0.4315, Val Acc: 0.8410\n",
      "Epoch 81/100, Loss: 0.2800, Acc: 0.8891, Val Loss: 0.4281, Val Acc: 0.8415\n",
      "Epoch 82/100, Loss: 0.2747, Acc: 0.8900, Val Loss: 0.4311, Val Acc: 0.8410\n",
      "Epoch 83/100, Loss: 0.2801, Acc: 0.8904, Val Loss: 0.4286, Val Acc: 0.8405\n",
      "Epoch 84/100, Loss: 0.2739, Acc: 0.8908, Val Loss: 0.4306, Val Acc: 0.8420\n",
      "Epoch 85/100, Loss: 0.2746, Acc: 0.8901, Val Loss: 0.4266, Val Acc: 0.8440\n",
      "Epoch 86/100, Loss: 0.2742, Acc: 0.8910, Val Loss: 0.4298, Val Acc: 0.8415\n",
      "Epoch 87/100, Loss: 0.2794, Acc: 0.8920, Val Loss: 0.4293, Val Acc: 0.8395\n",
      "Epoch 88/100, Loss: 0.2730, Acc: 0.8905, Val Loss: 0.4297, Val Acc: 0.8410\n",
      "Epoch 89/100, Loss: 0.2759, Acc: 0.8916, Val Loss: 0.4261, Val Acc: 0.8420\n",
      "Epoch 90/100, Loss: 0.2714, Acc: 0.8918, Val Loss: 0.4287, Val Acc: 0.8410\n",
      "Epoch 91/100, Loss: 0.2750, Acc: 0.8915, Val Loss: 0.4257, Val Acc: 0.8425\n",
      "Epoch 92/100, Loss: 0.2702, Acc: 0.8916, Val Loss: 0.4287, Val Acc: 0.8420\n",
      "Epoch 93/100, Loss: 0.2742, Acc: 0.8925, Val Loss: 0.4265, Val Acc: 0.8415\n",
      "Epoch 94/100, Loss: 0.2700, Acc: 0.8919, Val Loss: 0.4277, Val Acc: 0.8425\n",
      "Epoch 95/100, Loss: 0.2709, Acc: 0.8915, Val Loss: 0.4243, Val Acc: 0.8430\n",
      "Epoch 96/100, Loss: 0.2738, Acc: 0.8928, Val Loss: 0.4292, Val Acc: 0.8395\n",
      "Epoch 97/100, Loss: 0.2705, Acc: 0.8921, Val Loss: 0.4243, Val Acc: 0.8425\n",
      "Epoch 98/100, Loss: 0.2687, Acc: 0.8935, Val Loss: 0.4274, Val Acc: 0.8420\n",
      "Epoch 99/100, Loss: 0.2710, Acc: 0.8928, Val Loss: 0.4245, Val Acc: 0.8435\n",
      "Epoch 100/100, Loss: 0.2703, Acc: 0.8924, Val Loss: 0.4279, Val Acc: 0.8405\n",
      "✅ Accuracy : 0.8405\n",
      "✅ Precision: 0.7141\n",
      "✅ Recall   : 0.7066\n",
      "✅ F1 Score : 0.7076\n",
      "\n",
      "🔁 Running config 2/12\n",
      "Layers: [179, 7], Activations: ['softmax'], LR: 0.1\n",
      "Epoch 1/100, Loss: 3.3407, Acc: 0.5311, Val Loss: 2.7500, Val Acc: 0.6010\n",
      "Epoch 2/100, Loss: 2.2990, Acc: 0.6486, Val Loss: 2.2068, Val Acc: 0.6665\n",
      "Epoch 3/100, Loss: 1.9418, Acc: 0.6944, Val Loss: 2.1783, Val Acc: 0.6790\n",
      "Epoch 4/100, Loss: 1.7555, Acc: 0.7164, Val Loss: 2.3408, Val Acc: 0.6825\n",
      "Epoch 5/100, Loss: 1.6587, Acc: 0.7365, Val Loss: 2.2419, Val Acc: 0.7035\n",
      "Epoch 6/100, Loss: 1.5953, Acc: 0.7479, Val Loss: 1.9981, Val Acc: 0.7195\n",
      "Epoch 7/100, Loss: 1.5257, Acc: 0.7550, Val Loss: 1.7599, Val Acc: 0.7350\n",
      "Epoch 8/100, Loss: 1.4656, Acc: 0.7700, Val Loss: 1.8277, Val Acc: 0.7360\n",
      "Epoch 9/100, Loss: 1.4682, Acc: 0.7706, Val Loss: 1.9269, Val Acc: 0.7410\n",
      "Epoch 10/100, Loss: 1.3768, Acc: 0.7845, Val Loss: 1.8007, Val Acc: 0.7510\n",
      "Epoch 11/100, Loss: 1.3720, Acc: 0.7837, Val Loss: 1.7865, Val Acc: 0.7500\n",
      "Epoch 12/100, Loss: 1.4322, Acc: 0.7801, Val Loss: 1.9500, Val Acc: 0.7460\n",
      "Epoch 13/100, Loss: 1.3139, Acc: 0.7967, Val Loss: 1.7380, Val Acc: 0.7725\n",
      "Epoch 14/100, Loss: 1.4121, Acc: 0.7886, Val Loss: 1.9146, Val Acc: 0.7480\n",
      "Epoch 15/100, Loss: 1.3491, Acc: 0.7923, Val Loss: 1.7548, Val Acc: 0.7670\n",
      "Epoch 16/100, Loss: 1.3153, Acc: 0.7989, Val Loss: 1.7448, Val Acc: 0.7695\n",
      "Epoch 17/100, Loss: 1.2987, Acc: 0.8010, Val Loss: 1.6960, Val Acc: 0.7705\n",
      "Epoch 18/100, Loss: 1.2807, Acc: 0.8066, Val Loss: 1.7689, Val Acc: 0.7695\n",
      "Epoch 19/100, Loss: 1.3006, Acc: 0.8045, Val Loss: 1.7832, Val Acc: 0.7685\n",
      "Epoch 20/100, Loss: 1.2702, Acc: 0.8070, Val Loss: 1.7344, Val Acc: 0.7700\n",
      "Epoch 21/100, Loss: 1.2748, Acc: 0.8114, Val Loss: 1.8421, Val Acc: 0.7760\n",
      "Epoch 22/100, Loss: 1.2437, Acc: 0.8135, Val Loss: 1.7836, Val Acc: 0.7790\n",
      "Epoch 23/100, Loss: 1.2406, Acc: 0.8149, Val Loss: 1.7742, Val Acc: 0.7785\n",
      "Epoch 24/100, Loss: 1.2683, Acc: 0.8173, Val Loss: 1.7417, Val Acc: 0.7780\n",
      "Epoch 25/100, Loss: 1.1889, Acc: 0.8225, Val Loss: 1.8157, Val Acc: 0.7765\n",
      "Epoch 26/100, Loss: 1.2913, Acc: 0.8125, Val Loss: 1.9438, Val Acc: 0.7650\n",
      "Epoch 27/100, Loss: 1.1734, Acc: 0.8225, Val Loss: 1.6226, Val Acc: 0.7895\n",
      "Epoch 28/100, Loss: 1.2346, Acc: 0.8160, Val Loss: 1.8705, Val Acc: 0.7805\n",
      "Epoch 29/100, Loss: 1.1919, Acc: 0.8243, Val Loss: 1.7154, Val Acc: 0.7870\n",
      "Epoch 30/100, Loss: 1.1867, Acc: 0.8244, Val Loss: 1.7138, Val Acc: 0.7860\n",
      "Epoch 31/100, Loss: 1.2266, Acc: 0.8249, Val Loss: 2.0273, Val Acc: 0.7640\n",
      "Epoch 32/100, Loss: 1.2146, Acc: 0.8231, Val Loss: 1.7517, Val Acc: 0.7880\n",
      "Epoch 33/100, Loss: 1.1620, Acc: 0.8285, Val Loss: 1.7011, Val Acc: 0.7955\n",
      "Epoch 34/100, Loss: 1.1534, Acc: 0.8315, Val Loss: 1.6039, Val Acc: 0.7945\n",
      "Epoch 35/100, Loss: 1.2104, Acc: 0.8291, Val Loss: 1.6860, Val Acc: 0.7980\n",
      "Epoch 36/100, Loss: 1.1835, Acc: 0.8280, Val Loss: 1.7773, Val Acc: 0.7885\n",
      "Epoch 37/100, Loss: 1.1997, Acc: 0.8310, Val Loss: 1.9021, Val Acc: 0.7795\n",
      "Epoch 38/100, Loss: 1.1834, Acc: 0.8349, Val Loss: 1.6587, Val Acc: 0.8045\n",
      "Epoch 39/100, Loss: 1.1534, Acc: 0.8337, Val Loss: 1.7185, Val Acc: 0.7965\n",
      "Epoch 40/100, Loss: 1.1385, Acc: 0.8354, Val Loss: 1.6856, Val Acc: 0.8025\n",
      "Epoch 41/100, Loss: 1.1471, Acc: 0.8345, Val Loss: 1.6960, Val Acc: 0.8000\n",
      "Epoch 42/100, Loss: 1.0955, Acc: 0.8387, Val Loss: 1.6495, Val Acc: 0.8100\n",
      "Epoch 43/100, Loss: 1.1351, Acc: 0.8373, Val Loss: 1.7400, Val Acc: 0.8025\n",
      "Epoch 44/100, Loss: 1.0860, Acc: 0.8387, Val Loss: 1.6773, Val Acc: 0.8045\n",
      "Epoch 45/100, Loss: 1.1301, Acc: 0.8366, Val Loss: 1.6764, Val Acc: 0.7975\n",
      "Epoch 46/100, Loss: 1.1155, Acc: 0.8356, Val Loss: 1.5821, Val Acc: 0.8125\n",
      "Epoch 47/100, Loss: 1.1514, Acc: 0.8370, Val Loss: 1.9153, Val Acc: 0.7875\n",
      "Epoch 48/100, Loss: 1.1319, Acc: 0.8403, Val Loss: 1.7295, Val Acc: 0.7940\n",
      "Epoch 49/100, Loss: 1.1268, Acc: 0.8389, Val Loss: 1.7758, Val Acc: 0.8000\n",
      "Epoch 50/100, Loss: 1.1814, Acc: 0.8361, Val Loss: 1.6829, Val Acc: 0.8045\n",
      "Epoch 51/100, Loss: 1.1010, Acc: 0.8449, Val Loss: 1.8431, Val Acc: 0.8050\n",
      "Epoch 52/100, Loss: 1.1237, Acc: 0.8387, Val Loss: 1.7155, Val Acc: 0.8040\n",
      "Epoch 53/100, Loss: 1.1288, Acc: 0.8420, Val Loss: 1.7071, Val Acc: 0.8040\n",
      "Epoch 54/100, Loss: 1.1325, Acc: 0.8431, Val Loss: 1.6209, Val Acc: 0.8130\n",
      "Epoch 55/100, Loss: 1.1438, Acc: 0.8424, Val Loss: 1.7782, Val Acc: 0.8055\n",
      "Epoch 56/100, Loss: 1.1047, Acc: 0.8438, Val Loss: 1.7362, Val Acc: 0.8005\n",
      "Epoch 57/100, Loss: 1.1522, Acc: 0.8399, Val Loss: 1.8057, Val Acc: 0.8070\n",
      "Epoch 58/100, Loss: 1.1704, Acc: 0.8403, Val Loss: 1.7132, Val Acc: 0.8110\n",
      "Epoch 59/100, Loss: 1.0843, Acc: 0.8444, Val Loss: 1.6319, Val Acc: 0.8120\n",
      "Epoch 60/100, Loss: 1.1676, Acc: 0.8381, Val Loss: 1.8088, Val Acc: 0.7970\n",
      "Epoch 61/100, Loss: 1.1152, Acc: 0.8470, Val Loss: 1.6548, Val Acc: 0.8160\n",
      "Epoch 62/100, Loss: 1.1034, Acc: 0.8455, Val Loss: 1.6665, Val Acc: 0.8155\n",
      "Epoch 63/100, Loss: 1.1143, Acc: 0.8433, Val Loss: 1.7828, Val Acc: 0.8085\n",
      "Epoch 64/100, Loss: 1.0875, Acc: 0.8469, Val Loss: 1.6582, Val Acc: 0.8130\n",
      "Epoch 65/100, Loss: 1.1303, Acc: 0.8424, Val Loss: 1.8118, Val Acc: 0.8085\n",
      "Epoch 66/100, Loss: 1.0800, Acc: 0.8535, Val Loss: 1.6385, Val Acc: 0.8190\n",
      "Epoch 67/100, Loss: 1.0954, Acc: 0.8488, Val Loss: 1.8171, Val Acc: 0.8030\n",
      "Epoch 68/100, Loss: 1.1148, Acc: 0.8461, Val Loss: 1.7734, Val Acc: 0.8125\n",
      "Epoch 69/100, Loss: 1.1068, Acc: 0.8490, Val Loss: 1.8064, Val Acc: 0.8125\n",
      "Epoch 70/100, Loss: 1.0763, Acc: 0.8502, Val Loss: 1.7746, Val Acc: 0.8135\n",
      "Epoch 71/100, Loss: 1.0950, Acc: 0.8474, Val Loss: 1.7764, Val Acc: 0.8105\n",
      "Epoch 72/100, Loss: 1.0930, Acc: 0.8469, Val Loss: 1.6341, Val Acc: 0.8130\n",
      "Epoch 73/100, Loss: 1.1223, Acc: 0.8500, Val Loss: 1.7134, Val Acc: 0.8115\n",
      "Epoch 74/100, Loss: 1.1211, Acc: 0.8470, Val Loss: 1.7243, Val Acc: 0.8110\n",
      "Epoch 75/100, Loss: 1.0513, Acc: 0.8532, Val Loss: 1.8275, Val Acc: 0.8215\n",
      "Epoch 76/100, Loss: 1.1204, Acc: 0.8475, Val Loss: 1.8684, Val Acc: 0.8060\n",
      "Epoch 77/100, Loss: 1.1046, Acc: 0.8475, Val Loss: 1.8549, Val Acc: 0.8085\n",
      "Epoch 78/100, Loss: 1.0704, Acc: 0.8558, Val Loss: 1.7711, Val Acc: 0.8110\n",
      "Epoch 79/100, Loss: 1.1419, Acc: 0.8491, Val Loss: 1.8063, Val Acc: 0.8115\n",
      "Epoch 80/100, Loss: 1.0655, Acc: 0.8492, Val Loss: 1.6891, Val Acc: 0.8200\n",
      "Epoch 81/100, Loss: 1.1046, Acc: 0.8520, Val Loss: 1.6186, Val Acc: 0.8155\n",
      "Epoch 82/100, Loss: 1.0626, Acc: 0.8566, Val Loss: 1.6021, Val Acc: 0.8250\n",
      "Epoch 83/100, Loss: 1.1022, Acc: 0.8511, Val Loss: 1.5591, Val Acc: 0.8210\n",
      "Epoch 84/100, Loss: 1.1368, Acc: 0.8452, Val Loss: 1.7159, Val Acc: 0.8205\n",
      "Epoch 85/100, Loss: 1.0811, Acc: 0.8530, Val Loss: 1.6808, Val Acc: 0.8195\n",
      "Epoch 86/100, Loss: 1.1063, Acc: 0.8520, Val Loss: 1.7392, Val Acc: 0.8170\n",
      "Epoch 87/100, Loss: 1.0526, Acc: 0.8566, Val Loss: 1.7767, Val Acc: 0.8110\n",
      "Epoch 88/100, Loss: 1.1143, Acc: 0.8474, Val Loss: 1.7580, Val Acc: 0.8200\n",
      "Epoch 89/100, Loss: 1.0806, Acc: 0.8540, Val Loss: 1.8846, Val Acc: 0.8220\n",
      "Epoch 90/100, Loss: 1.0929, Acc: 0.8512, Val Loss: 1.7279, Val Acc: 0.8210\n",
      "Epoch 91/100, Loss: 1.1114, Acc: 0.8532, Val Loss: 1.7389, Val Acc: 0.8200\n",
      "Epoch 92/100, Loss: 1.0470, Acc: 0.8549, Val Loss: 1.6937, Val Acc: 0.8225\n",
      "Epoch 93/100, Loss: 1.0857, Acc: 0.8564, Val Loss: 1.6873, Val Acc: 0.8285\n",
      "Epoch 94/100, Loss: 1.0346, Acc: 0.8578, Val Loss: 1.7769, Val Acc: 0.8190\n",
      "Epoch 95/100, Loss: 1.0715, Acc: 0.8561, Val Loss: 1.8740, Val Acc: 0.8165\n",
      "Epoch 96/100, Loss: 1.0621, Acc: 0.8564, Val Loss: 1.6710, Val Acc: 0.8265\n",
      "Epoch 97/100, Loss: 1.0749, Acc: 0.8584, Val Loss: 1.8272, Val Acc: 0.8130\n",
      "Epoch 98/100, Loss: 1.0716, Acc: 0.8546, Val Loss: 1.8269, Val Acc: 0.8240\n",
      "Epoch 99/100, Loss: 1.0712, Acc: 0.8582, Val Loss: 1.7137, Val Acc: 0.8160\n",
      "Epoch 100/100, Loss: 1.0576, Acc: 0.8521, Val Loss: 1.6601, Val Acc: 0.8250\n",
      "✅ Accuracy : 0.8250\n",
      "✅ Precision: 0.7009\n",
      "✅ Recall   : 0.7101\n",
      "✅ F1 Score : 0.7005\n",
      "\n",
      "🔁 Running config 3/12\n",
      "Layers: [179, 16, 7], Activations: ['relu', 'softmax'], LR: 0.01\n",
      "Epoch 1/100, Loss: 2.2660, Acc: 0.4189, Val Loss: 1.1467, Val Acc: 0.5555\n",
      "Epoch 2/100, Loss: 1.0002, Acc: 0.6072, Val Loss: 0.7684, Val Acc: 0.6905\n",
      "Epoch 3/100, Loss: 0.8146, Acc: 0.6921, Val Loss: 0.7356, Val Acc: 0.7210\n",
      "Epoch 4/100, Loss: 0.7001, Acc: 0.7298, Val Loss: 0.6749, Val Acc: 0.7395\n",
      "Epoch 5/100, Loss: 0.6535, Acc: 0.7474, Val Loss: 0.6213, Val Acc: 0.7550\n",
      "Epoch 6/100, Loss: 0.6015, Acc: 0.7639, Val Loss: 0.5983, Val Acc: 0.7670\n",
      "Epoch 7/100, Loss: 0.5630, Acc: 0.7795, Val Loss: 0.6116, Val Acc: 0.7650\n",
      "Epoch 8/100, Loss: 0.5368, Acc: 0.7885, Val Loss: 0.5898, Val Acc: 0.7715\n",
      "Epoch 9/100, Loss: 0.5170, Acc: 0.7967, Val Loss: 0.5746, Val Acc: 0.7745\n",
      "Epoch 10/100, Loss: 0.5012, Acc: 0.8039, Val Loss: 0.5796, Val Acc: 0.7820\n",
      "Epoch 11/100, Loss: 0.4925, Acc: 0.8065, Val Loss: 0.5645, Val Acc: 0.7860\n",
      "Epoch 12/100, Loss: 0.4822, Acc: 0.8119, Val Loss: 0.5631, Val Acc: 0.7865\n",
      "Epoch 13/100, Loss: 0.4692, Acc: 0.8156, Val Loss: 0.5525, Val Acc: 0.7895\n",
      "Epoch 14/100, Loss: 0.4691, Acc: 0.8151, Val Loss: 0.5647, Val Acc: 0.7880\n",
      "Epoch 15/100, Loss: 0.4530, Acc: 0.8235, Val Loss: 0.5394, Val Acc: 0.8065\n",
      "Epoch 16/100, Loss: 0.4455, Acc: 0.8247, Val Loss: 0.5286, Val Acc: 0.8090\n",
      "Epoch 17/100, Loss: 0.4320, Acc: 0.8273, Val Loss: 0.5405, Val Acc: 0.8065\n",
      "Epoch 18/100, Loss: 0.4303, Acc: 0.8311, Val Loss: 0.5354, Val Acc: 0.8075\n",
      "Epoch 19/100, Loss: 0.4219, Acc: 0.8347, Val Loss: 0.5509, Val Acc: 0.7995\n",
      "Epoch 20/100, Loss: 0.4128, Acc: 0.8359, Val Loss: 0.5579, Val Acc: 0.7985\n",
      "Epoch 21/100, Loss: 0.4064, Acc: 0.8384, Val Loss: 0.5292, Val Acc: 0.7965\n",
      "Epoch 22/100, Loss: 0.3975, Acc: 0.8419, Val Loss: 0.5513, Val Acc: 0.8030\n",
      "Epoch 23/100, Loss: 0.3915, Acc: 0.8455, Val Loss: 0.5392, Val Acc: 0.8040\n",
      "Epoch 24/100, Loss: 0.3858, Acc: 0.8480, Val Loss: 0.5336, Val Acc: 0.8090\n",
      "Epoch 25/100, Loss: 0.3813, Acc: 0.8502, Val Loss: 0.5258, Val Acc: 0.8040\n",
      "Epoch 26/100, Loss: 0.3800, Acc: 0.8528, Val Loss: 0.5104, Val Acc: 0.8145\n",
      "Epoch 27/100, Loss: 0.3846, Acc: 0.8520, Val Loss: 0.5083, Val Acc: 0.8110\n",
      "Epoch 28/100, Loss: 0.3716, Acc: 0.8578, Val Loss: 0.5201, Val Acc: 0.8105\n",
      "Epoch 29/100, Loss: 0.3725, Acc: 0.8555, Val Loss: 0.5344, Val Acc: 0.8070\n",
      "Epoch 30/100, Loss: 0.3651, Acc: 0.8588, Val Loss: 0.5595, Val Acc: 0.8110\n",
      "Epoch 31/100, Loss: 0.3646, Acc: 0.8586, Val Loss: 0.5527, Val Acc: 0.8140\n",
      "Epoch 32/100, Loss: 0.3599, Acc: 0.8611, Val Loss: 0.5433, Val Acc: 0.8195\n",
      "Epoch 33/100, Loss: 0.3524, Acc: 0.8675, Val Loss: 0.5391, Val Acc: 0.8195\n",
      "Epoch 34/100, Loss: 0.3458, Acc: 0.8670, Val Loss: 0.5430, Val Acc: 0.8215\n",
      "Epoch 35/100, Loss: 0.3406, Acc: 0.8710, Val Loss: 0.5473, Val Acc: 0.8195\n",
      "Epoch 36/100, Loss: 0.3738, Acc: 0.8595, Val Loss: 0.5528, Val Acc: 0.8200\n",
      "Epoch 37/100, Loss: 0.3532, Acc: 0.8651, Val Loss: 0.5569, Val Acc: 0.8230\n",
      "Epoch 38/100, Loss: 0.3813, Acc: 0.8541, Val Loss: 0.5470, Val Acc: 0.8290\n",
      "Epoch 39/100, Loss: 0.3585, Acc: 0.8676, Val Loss: 0.5418, Val Acc: 0.8210\n",
      "Epoch 40/100, Loss: 0.3438, Acc: 0.8714, Val Loss: 0.5331, Val Acc: 0.8305\n",
      "Epoch 41/100, Loss: 0.3731, Acc: 0.8621, Val Loss: 0.4979, Val Acc: 0.8370\n",
      "Epoch 42/100, Loss: 0.3483, Acc: 0.8690, Val Loss: 0.4875, Val Acc: 0.8440\n",
      "Epoch 43/100, Loss: 0.3236, Acc: 0.8800, Val Loss: 0.5058, Val Acc: 0.8365\n",
      "Epoch 44/100, Loss: 0.3280, Acc: 0.8792, Val Loss: 0.5715, Val Acc: 0.8175\n",
      "Epoch 45/100, Loss: 0.3607, Acc: 0.8725, Val Loss: 0.5008, Val Acc: 0.8360\n",
      "Epoch 46/100, Loss: 0.3228, Acc: 0.8800, Val Loss: 0.5319, Val Acc: 0.8320\n",
      "Epoch 47/100, Loss: 0.3116, Acc: 0.8854, Val Loss: 0.4896, Val Acc: 0.8405\n",
      "Epoch 48/100, Loss: 0.3053, Acc: 0.8872, Val Loss: 0.5528, Val Acc: 0.8155\n",
      "Epoch 49/100, Loss: 0.3055, Acc: 0.8891, Val Loss: 0.4882, Val Acc: 0.8400\n",
      "Epoch 50/100, Loss: 0.3083, Acc: 0.8916, Val Loss: 0.4817, Val Acc: 0.8390\n",
      "Epoch 51/100, Loss: 0.3202, Acc: 0.8860, Val Loss: 0.5255, Val Acc: 0.8285\n",
      "Epoch 52/100, Loss: 0.3091, Acc: 0.8870, Val Loss: 0.5071, Val Acc: 0.8350\n",
      "Epoch 53/100, Loss: 0.3158, Acc: 0.8855, Val Loss: 0.5588, Val Acc: 0.8240\n",
      "Epoch 54/100, Loss: 0.3094, Acc: 0.8896, Val Loss: 0.5871, Val Acc: 0.8195\n",
      "Epoch 55/100, Loss: 0.3017, Acc: 0.8925, Val Loss: 0.5685, Val Acc: 0.8300\n",
      "Epoch 56/100, Loss: 0.3024, Acc: 0.8929, Val Loss: 0.5342, Val Acc: 0.8320\n",
      "Epoch 57/100, Loss: 0.2917, Acc: 0.8985, Val Loss: 0.5776, Val Acc: 0.8290\n",
      "Epoch 58/100, Loss: 0.2967, Acc: 0.8962, Val Loss: 0.6010, Val Acc: 0.8265\n",
      "Epoch 59/100, Loss: 0.3016, Acc: 0.8962, Val Loss: 0.6010, Val Acc: 0.8210\n",
      "Epoch 60/100, Loss: 0.2952, Acc: 0.8996, Val Loss: 0.5452, Val Acc: 0.8370\n",
      "Epoch 61/100, Loss: 0.2922, Acc: 0.9006, Val Loss: 0.5289, Val Acc: 0.8385\n",
      "Epoch 62/100, Loss: 0.3078, Acc: 0.8910, Val Loss: 0.5218, Val Acc: 0.8395\n",
      "Epoch 63/100, Loss: 0.2982, Acc: 0.8958, Val Loss: 0.5526, Val Acc: 0.8355\n",
      "Epoch 64/100, Loss: 0.2844, Acc: 0.9011, Val Loss: 0.5785, Val Acc: 0.8315\n",
      "Epoch 65/100, Loss: 0.2795, Acc: 0.9038, Val Loss: 0.5186, Val Acc: 0.8395\n",
      "Epoch 66/100, Loss: 0.2895, Acc: 0.8994, Val Loss: 0.5281, Val Acc: 0.8400\n",
      "Epoch 67/100, Loss: 0.2879, Acc: 0.8960, Val Loss: 0.6203, Val Acc: 0.8325\n",
      "Epoch 68/100, Loss: 0.2973, Acc: 0.8988, Val Loss: 0.5921, Val Acc: 0.8295\n",
      "Epoch 69/100, Loss: 0.2861, Acc: 0.9015, Val Loss: 0.6105, Val Acc: 0.8265\n",
      "Epoch 70/100, Loss: 0.2954, Acc: 0.8966, Val Loss: 0.7059, Val Acc: 0.8090\n",
      "Epoch 71/100, Loss: 0.2912, Acc: 0.8965, Val Loss: 0.5677, Val Acc: 0.8390\n",
      "Epoch 72/100, Loss: 0.2869, Acc: 0.8971, Val Loss: 0.6451, Val Acc: 0.8195\n",
      "Epoch 73/100, Loss: 0.2873, Acc: 0.8988, Val Loss: 0.6086, Val Acc: 0.8260\n",
      "Epoch 74/100, Loss: 0.2803, Acc: 0.8975, Val Loss: 0.5458, Val Acc: 0.8410\n",
      "Epoch 75/100, Loss: 0.2892, Acc: 0.9006, Val Loss: 0.5625, Val Acc: 0.8325\n",
      "Epoch 76/100, Loss: 0.2945, Acc: 0.8946, Val Loss: 0.6064, Val Acc: 0.8400\n",
      "Epoch 77/100, Loss: 0.2991, Acc: 0.8962, Val Loss: 0.6117, Val Acc: 0.8330\n",
      "Epoch 78/100, Loss: 0.2996, Acc: 0.8965, Val Loss: 0.5826, Val Acc: 0.8420\n",
      "Epoch 79/100, Loss: 0.2982, Acc: 0.8982, Val Loss: 0.5487, Val Acc: 0.8455\n",
      "Epoch 80/100, Loss: 0.3354, Acc: 0.8905, Val Loss: 0.7587, Val Acc: 0.8245\n",
      "Epoch 81/100, Loss: 0.3042, Acc: 0.8908, Val Loss: 0.4886, Val Acc: 0.8525\n",
      "Epoch 82/100, Loss: 0.3079, Acc: 0.8952, Val Loss: 0.6110, Val Acc: 0.8345\n",
      "Epoch 83/100, Loss: 0.3355, Acc: 0.8932, Val Loss: 0.5787, Val Acc: 0.8355\n",
      "Epoch 84/100, Loss: 0.2985, Acc: 0.8951, Val Loss: 0.5631, Val Acc: 0.8475\n",
      "Epoch 85/100, Loss: 0.2826, Acc: 0.9018, Val Loss: 0.6393, Val Acc: 0.8280\n",
      "Epoch 86/100, Loss: 0.2866, Acc: 0.8989, Val Loss: 0.6216, Val Acc: 0.8355\n",
      "Epoch 87/100, Loss: 0.2775, Acc: 0.9006, Val Loss: 0.5422, Val Acc: 0.8465\n",
      "Epoch 88/100, Loss: 0.2676, Acc: 0.9035, Val Loss: 0.5538, Val Acc: 0.8335\n",
      "Epoch 89/100, Loss: 0.2690, Acc: 0.9009, Val Loss: 0.5495, Val Acc: 0.8465\n",
      "Epoch 90/100, Loss: 0.2650, Acc: 0.9046, Val Loss: 0.5346, Val Acc: 0.8445\n",
      "Epoch 91/100, Loss: 0.2627, Acc: 0.9031, Val Loss: 0.5522, Val Acc: 0.8355\n",
      "Epoch 92/100, Loss: 0.2745, Acc: 0.9019, Val Loss: 0.5809, Val Acc: 0.8355\n",
      "Epoch 93/100, Loss: 0.2764, Acc: 0.8999, Val Loss: 0.5761, Val Acc: 0.8415\n",
      "Epoch 94/100, Loss: 0.2987, Acc: 0.8951, Val Loss: 0.6401, Val Acc: 0.8410\n",
      "Epoch 95/100, Loss: 0.2726, Acc: 0.9044, Val Loss: 0.6291, Val Acc: 0.8355\n",
      "Epoch 96/100, Loss: 0.2721, Acc: 0.9048, Val Loss: 0.5890, Val Acc: 0.8410\n",
      "Epoch 97/100, Loss: 0.2719, Acc: 0.9012, Val Loss: 0.5681, Val Acc: 0.8455\n",
      "Epoch 98/100, Loss: 0.2699, Acc: 0.9074, Val Loss: 0.6156, Val Acc: 0.8345\n",
      "Epoch 99/100, Loss: 0.2657, Acc: 0.9025, Val Loss: 0.5900, Val Acc: 0.8415\n",
      "Epoch 100/100, Loss: 0.2693, Acc: 0.9034, Val Loss: 0.6019, Val Acc: 0.8520\n",
      "✅ Accuracy : 0.8520\n",
      "✅ Precision: 0.7165\n",
      "✅ Recall   : 0.7463\n",
      "✅ F1 Score : 0.7011\n",
      "\n",
      "🔁 Running config 4/12\n",
      "Layers: [179, 16, 7], Activations: ['tanh', 'softmax'], LR: 0.01\n",
      "Epoch 1/100, Loss: 1.6512, Acc: 0.3730, Val Loss: 1.2492, Val Acc: 0.4565\n",
      "Epoch 2/100, Loss: 1.1422, Acc: 0.4950, Val Loss: 1.0876, Val Acc: 0.5190\n",
      "Epoch 3/100, Loss: 0.9861, Acc: 0.5630, Val Loss: 0.9623, Val Acc: 0.5655\n",
      "Epoch 4/100, Loss: 0.8724, Acc: 0.6195, Val Loss: 0.8665, Val Acc: 0.6160\n",
      "Epoch 5/100, Loss: 0.7821, Acc: 0.6624, Val Loss: 0.7999, Val Acc: 0.6450\n",
      "Epoch 6/100, Loss: 0.7166, Acc: 0.6907, Val Loss: 0.7533, Val Acc: 0.6685\n",
      "Epoch 7/100, Loss: 0.6630, Acc: 0.7171, Val Loss: 0.7177, Val Acc: 0.6880\n",
      "Epoch 8/100, Loss: 0.6183, Acc: 0.7404, Val Loss: 0.6875, Val Acc: 0.7130\n",
      "Epoch 9/100, Loss: 0.5829, Acc: 0.7588, Val Loss: 0.6654, Val Acc: 0.7200\n",
      "Epoch 10/100, Loss: 0.5527, Acc: 0.7742, Val Loss: 0.6404, Val Acc: 0.7290\n",
      "Epoch 11/100, Loss: 0.5270, Acc: 0.7800, Val Loss: 0.6235, Val Acc: 0.7320\n",
      "Epoch 12/100, Loss: 0.5057, Acc: 0.7874, Val Loss: 0.6109, Val Acc: 0.7405\n",
      "Epoch 13/100, Loss: 0.4878, Acc: 0.7985, Val Loss: 0.6013, Val Acc: 0.7460\n",
      "Epoch 14/100, Loss: 0.4705, Acc: 0.8059, Val Loss: 0.5898, Val Acc: 0.7465\n",
      "Epoch 15/100, Loss: 0.4543, Acc: 0.8149, Val Loss: 0.5823, Val Acc: 0.7545\n",
      "Epoch 16/100, Loss: 0.4437, Acc: 0.8194, Val Loss: 0.5788, Val Acc: 0.7565\n",
      "Epoch 17/100, Loss: 0.4335, Acc: 0.8231, Val Loss: 0.5741, Val Acc: 0.7585\n",
      "Epoch 18/100, Loss: 0.4249, Acc: 0.8283, Val Loss: 0.5716, Val Acc: 0.7585\n",
      "Epoch 19/100, Loss: 0.4172, Acc: 0.8319, Val Loss: 0.5699, Val Acc: 0.7645\n",
      "Epoch 20/100, Loss: 0.4106, Acc: 0.8345, Val Loss: 0.5715, Val Acc: 0.7610\n",
      "Epoch 21/100, Loss: 0.4022, Acc: 0.8377, Val Loss: 0.5725, Val Acc: 0.7555\n",
      "Epoch 22/100, Loss: 0.3950, Acc: 0.8413, Val Loss: 0.5743, Val Acc: 0.7595\n",
      "Epoch 23/100, Loss: 0.3899, Acc: 0.8430, Val Loss: 0.5766, Val Acc: 0.7580\n",
      "Epoch 24/100, Loss: 0.3838, Acc: 0.8454, Val Loss: 0.5800, Val Acc: 0.7560\n",
      "Epoch 25/100, Loss: 0.3792, Acc: 0.8468, Val Loss: 0.5793, Val Acc: 0.7605\n",
      "Epoch 26/100, Loss: 0.3727, Acc: 0.8474, Val Loss: 0.5819, Val Acc: 0.7590\n",
      "Epoch 27/100, Loss: 0.3679, Acc: 0.8499, Val Loss: 0.5837, Val Acc: 0.7610\n",
      "Epoch 28/100, Loss: 0.3618, Acc: 0.8534, Val Loss: 0.5832, Val Acc: 0.7615\n",
      "Epoch 29/100, Loss: 0.3574, Acc: 0.8566, Val Loss: 0.5854, Val Acc: 0.7610\n",
      "Epoch 30/100, Loss: 0.3516, Acc: 0.8604, Val Loss: 0.5895, Val Acc: 0.7605\n",
      "Epoch 31/100, Loss: 0.3474, Acc: 0.8598, Val Loss: 0.5894, Val Acc: 0.7590\n",
      "Epoch 32/100, Loss: 0.3423, Acc: 0.8621, Val Loss: 0.5927, Val Acc: 0.7595\n",
      "Epoch 33/100, Loss: 0.3395, Acc: 0.8645, Val Loss: 0.5961, Val Acc: 0.7555\n",
      "Epoch 34/100, Loss: 0.3356, Acc: 0.8664, Val Loss: 0.5969, Val Acc: 0.7560\n",
      "Epoch 35/100, Loss: 0.3316, Acc: 0.8704, Val Loss: 0.5977, Val Acc: 0.7610\n",
      "Epoch 36/100, Loss: 0.3274, Acc: 0.8718, Val Loss: 0.5996, Val Acc: 0.7570\n",
      "Epoch 37/100, Loss: 0.3247, Acc: 0.8740, Val Loss: 0.6000, Val Acc: 0.7580\n",
      "Epoch 38/100, Loss: 0.3221, Acc: 0.8754, Val Loss: 0.6103, Val Acc: 0.7525\n",
      "Epoch 39/100, Loss: 0.3199, Acc: 0.8771, Val Loss: 0.6102, Val Acc: 0.7545\n",
      "Epoch 40/100, Loss: 0.3160, Acc: 0.8779, Val Loss: 0.6119, Val Acc: 0.7515\n",
      "Epoch 41/100, Loss: 0.3148, Acc: 0.8788, Val Loss: 0.6126, Val Acc: 0.7560\n",
      "Epoch 42/100, Loss: 0.3106, Acc: 0.8808, Val Loss: 0.6145, Val Acc: 0.7600\n",
      "Epoch 43/100, Loss: 0.3085, Acc: 0.8830, Val Loss: 0.6188, Val Acc: 0.7580\n",
      "Epoch 44/100, Loss: 0.3070, Acc: 0.8828, Val Loss: 0.6196, Val Acc: 0.7605\n",
      "Epoch 45/100, Loss: 0.3033, Acc: 0.8844, Val Loss: 0.6221, Val Acc: 0.7580\n",
      "Epoch 46/100, Loss: 0.3020, Acc: 0.8834, Val Loss: 0.6239, Val Acc: 0.7565\n",
      "Epoch 47/100, Loss: 0.2992, Acc: 0.8851, Val Loss: 0.6267, Val Acc: 0.7565\n",
      "Epoch 48/100, Loss: 0.2985, Acc: 0.8831, Val Loss: 0.6259, Val Acc: 0.7570\n",
      "Epoch 49/100, Loss: 0.2973, Acc: 0.8846, Val Loss: 0.6320, Val Acc: 0.7550\n",
      "Epoch 50/100, Loss: 0.2956, Acc: 0.8858, Val Loss: 0.6311, Val Acc: 0.7590\n",
      "Epoch 51/100, Loss: 0.2931, Acc: 0.8864, Val Loss: 0.6359, Val Acc: 0.7610\n",
      "Epoch 52/100, Loss: 0.2912, Acc: 0.8898, Val Loss: 0.6332, Val Acc: 0.7625\n",
      "Epoch 53/100, Loss: 0.2906, Acc: 0.8885, Val Loss: 0.6407, Val Acc: 0.7570\n",
      "Epoch 54/100, Loss: 0.2862, Acc: 0.8910, Val Loss: 0.6397, Val Acc: 0.7620\n",
      "Epoch 55/100, Loss: 0.2881, Acc: 0.8891, Val Loss: 0.6433, Val Acc: 0.7575\n",
      "Epoch 56/100, Loss: 0.2862, Acc: 0.8900, Val Loss: 0.6393, Val Acc: 0.7610\n",
      "Epoch 57/100, Loss: 0.2822, Acc: 0.8900, Val Loss: 0.6416, Val Acc: 0.7635\n",
      "Epoch 58/100, Loss: 0.2801, Acc: 0.8930, Val Loss: 0.6412, Val Acc: 0.7675\n",
      "Epoch 59/100, Loss: 0.2781, Acc: 0.8941, Val Loss: 0.6418, Val Acc: 0.7710\n",
      "Epoch 60/100, Loss: 0.2765, Acc: 0.8941, Val Loss: 0.6418, Val Acc: 0.7705\n",
      "Epoch 61/100, Loss: 0.2735, Acc: 0.8962, Val Loss: 0.6472, Val Acc: 0.7710\n",
      "Epoch 62/100, Loss: 0.2743, Acc: 0.8948, Val Loss: 0.6472, Val Acc: 0.7725\n",
      "Epoch 63/100, Loss: 0.2707, Acc: 0.8981, Val Loss: 0.6452, Val Acc: 0.7715\n",
      "Epoch 64/100, Loss: 0.2683, Acc: 0.8979, Val Loss: 0.6462, Val Acc: 0.7740\n",
      "Epoch 65/100, Loss: 0.2669, Acc: 0.8998, Val Loss: 0.6450, Val Acc: 0.7720\n",
      "Epoch 66/100, Loss: 0.2657, Acc: 0.8999, Val Loss: 0.6463, Val Acc: 0.7715\n",
      "Epoch 67/100, Loss: 0.2644, Acc: 0.9012, Val Loss: 0.6470, Val Acc: 0.7725\n",
      "Epoch 68/100, Loss: 0.2617, Acc: 0.9024, Val Loss: 0.6452, Val Acc: 0.7745\n",
      "Epoch 69/100, Loss: 0.2599, Acc: 0.9032, Val Loss: 0.6488, Val Acc: 0.7740\n",
      "Epoch 70/100, Loss: 0.2601, Acc: 0.9039, Val Loss: 0.6511, Val Acc: 0.7685\n",
      "Epoch 71/100, Loss: 0.2595, Acc: 0.9034, Val Loss: 0.6541, Val Acc: 0.7700\n",
      "Epoch 72/100, Loss: 0.2571, Acc: 0.9051, Val Loss: 0.6535, Val Acc: 0.7700\n",
      "Epoch 73/100, Loss: 0.2596, Acc: 0.9030, Val Loss: 0.6578, Val Acc: 0.7650\n",
      "Epoch 74/100, Loss: 0.2573, Acc: 0.9045, Val Loss: 0.6556, Val Acc: 0.7705\n",
      "Epoch 75/100, Loss: 0.2551, Acc: 0.9064, Val Loss: 0.6539, Val Acc: 0.7715\n",
      "Epoch 76/100, Loss: 0.2550, Acc: 0.9067, Val Loss: 0.6532, Val Acc: 0.7690\n",
      "Epoch 77/100, Loss: 0.2563, Acc: 0.9066, Val Loss: 0.6495, Val Acc: 0.7710\n",
      "Epoch 78/100, Loss: 0.2536, Acc: 0.9069, Val Loss: 0.6677, Val Acc: 0.7680\n",
      "Epoch 79/100, Loss: 0.2514, Acc: 0.9074, Val Loss: 0.6671, Val Acc: 0.7695\n",
      "Epoch 80/100, Loss: 0.2532, Acc: 0.9054, Val Loss: 0.6631, Val Acc: 0.7700\n",
      "Epoch 81/100, Loss: 0.2504, Acc: 0.9075, Val Loss: 0.6667, Val Acc: 0.7685\n",
      "Epoch 82/100, Loss: 0.2475, Acc: 0.9087, Val Loss: 0.6719, Val Acc: 0.7705\n",
      "Epoch 83/100, Loss: 0.2456, Acc: 0.9087, Val Loss: 0.6796, Val Acc: 0.7675\n",
      "Epoch 84/100, Loss: 0.2494, Acc: 0.9074, Val Loss: 0.6780, Val Acc: 0.7690\n",
      "Epoch 85/100, Loss: 0.2487, Acc: 0.9064, Val Loss: 0.6846, Val Acc: 0.7695\n",
      "Epoch 86/100, Loss: 0.2461, Acc: 0.9087, Val Loss: 0.6794, Val Acc: 0.7650\n",
      "Epoch 87/100, Loss: 0.2427, Acc: 0.9100, Val Loss: 0.6805, Val Acc: 0.7695\n",
      "Epoch 88/100, Loss: 0.2418, Acc: 0.9105, Val Loss: 0.6837, Val Acc: 0.7655\n",
      "Epoch 89/100, Loss: 0.2412, Acc: 0.9103, Val Loss: 0.6824, Val Acc: 0.7680\n",
      "Epoch 90/100, Loss: 0.2385, Acc: 0.9126, Val Loss: 0.6861, Val Acc: 0.7700\n",
      "Epoch 91/100, Loss: 0.2369, Acc: 0.9119, Val Loss: 0.6880, Val Acc: 0.7685\n",
      "Epoch 92/100, Loss: 0.2383, Acc: 0.9131, Val Loss: 0.6992, Val Acc: 0.7665\n",
      "Epoch 93/100, Loss: 0.2478, Acc: 0.9067, Val Loss: 0.6992, Val Acc: 0.7610\n",
      "Epoch 94/100, Loss: 0.2420, Acc: 0.9107, Val Loss: 0.6931, Val Acc: 0.7680\n",
      "Epoch 95/100, Loss: 0.2350, Acc: 0.9144, Val Loss: 0.6970, Val Acc: 0.7675\n",
      "Epoch 96/100, Loss: 0.2312, Acc: 0.9154, Val Loss: 0.6943, Val Acc: 0.7700\n",
      "Epoch 97/100, Loss: 0.2303, Acc: 0.9174, Val Loss: 0.6944, Val Acc: 0.7705\n",
      "Epoch 98/100, Loss: 0.2299, Acc: 0.9177, Val Loss: 0.6976, Val Acc: 0.7760\n",
      "Epoch 99/100, Loss: 0.2294, Acc: 0.9167, Val Loss: 0.7012, Val Acc: 0.7715\n",
      "Epoch 100/100, Loss: 0.2274, Acc: 0.9166, Val Loss: 0.7003, Val Acc: 0.7710\n",
      "✅ Accuracy : 0.7710\n",
      "✅ Precision: 0.5636\n",
      "✅ Recall   : 0.5724\n",
      "✅ F1 Score : 0.5676\n",
      "\n",
      "🔁 Running config 5/12\n",
      "Layers: [179, 16, 7], Activations: ['relu', 'softmax'], LR: 0.1\n",
      "Epoch 1/100, Loss: 1.9658, Acc: 0.2953, Val Loss: 1.6761, Val Acc: 0.2880\n",
      "Epoch 2/100, Loss: 1.6050, Acc: 0.2915, Val Loss: 1.6561, Val Acc: 0.2930\n",
      "Epoch 3/100, Loss: 1.6019, Acc: 0.2920, Val Loss: 1.6890, Val Acc: 0.2925\n",
      "Epoch 4/100, Loss: 1.6023, Acc: 0.2921, Val Loss: 1.6560, Val Acc: 0.2935\n",
      "Epoch 5/100, Loss: 1.6001, Acc: 0.2925, Val Loss: 1.6702, Val Acc: 0.2935\n",
      "Epoch 6/100, Loss: 1.6006, Acc: 0.2928, Val Loss: 1.6639, Val Acc: 0.2935\n",
      "Epoch 7/100, Loss: 1.6001, Acc: 0.2930, Val Loss: 1.6570, Val Acc: 0.2935\n",
      "Epoch 8/100, Loss: 1.6001, Acc: 0.2930, Val Loss: 1.6589, Val Acc: 0.2930\n",
      "Epoch 9/100, Loss: 1.6004, Acc: 0.2928, Val Loss: 1.6566, Val Acc: 0.2930\n",
      "Epoch 10/100, Loss: 1.6001, Acc: 0.2931, Val Loss: 1.6569, Val Acc: 0.2935\n",
      "Epoch 11/100, Loss: 1.6003, Acc: 0.2934, Val Loss: 1.6577, Val Acc: 0.2930\n",
      "Epoch 12/100, Loss: 1.6002, Acc: 0.2931, Val Loss: 1.6580, Val Acc: 0.2930\n",
      "Epoch 13/100, Loss: 1.6002, Acc: 0.2928, Val Loss: 1.6582, Val Acc: 0.2930\n",
      "Epoch 14/100, Loss: 1.6001, Acc: 0.2930, Val Loss: 1.6584, Val Acc: 0.2930\n",
      "Epoch 15/100, Loss: 1.6002, Acc: 0.2928, Val Loss: 1.6598, Val Acc: 0.2930\n",
      "Epoch 16/100, Loss: 1.6007, Acc: 0.2924, Val Loss: 1.6592, Val Acc: 0.2930\n",
      "Epoch 17/100, Loss: 1.6006, Acc: 0.2928, Val Loss: 1.6593, Val Acc: 0.2930\n",
      "Epoch 18/100, Loss: 1.6005, Acc: 0.2931, Val Loss: 1.6594, Val Acc: 0.2930\n",
      "Epoch 19/100, Loss: 1.6005, Acc: 0.2933, Val Loss: 1.6595, Val Acc: 0.2930\n",
      "Epoch 20/100, Loss: 1.6005, Acc: 0.2931, Val Loss: 1.6595, Val Acc: 0.2930\n",
      "Epoch 21/100, Loss: 1.6005, Acc: 0.2933, Val Loss: 1.6596, Val Acc: 0.2930\n",
      "Epoch 22/100, Loss: 1.6005, Acc: 0.2933, Val Loss: 1.6597, Val Acc: 0.2930\n",
      "Epoch 23/100, Loss: 1.6005, Acc: 0.2933, Val Loss: 1.6597, Val Acc: 0.2930\n",
      "Epoch 24/100, Loss: 1.6005, Acc: 0.2933, Val Loss: 1.6598, Val Acc: 0.2930\n",
      "Epoch 25/100, Loss: 1.6005, Acc: 0.2933, Val Loss: 1.6599, Val Acc: 0.2930\n",
      "Epoch 26/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6599, Val Acc: 0.2930\n",
      "Epoch 27/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6600, Val Acc: 0.2930\n",
      "Epoch 28/100, Loss: 1.6005, Acc: 0.2933, Val Loss: 1.6600, Val Acc: 0.2930\n",
      "Epoch 29/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6601, Val Acc: 0.2930\n",
      "Epoch 30/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6601, Val Acc: 0.2930\n",
      "Epoch 31/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6602, Val Acc: 0.2930\n",
      "Epoch 32/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6602, Val Acc: 0.2930\n",
      "Epoch 33/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6602, Val Acc: 0.2930\n",
      "Epoch 34/100, Loss: 1.6004, Acc: 0.2931, Val Loss: 1.6603, Val Acc: 0.2930\n",
      "Epoch 35/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6603, Val Acc: 0.2930\n",
      "Epoch 36/100, Loss: 1.6004, Acc: 0.2933, Val Loss: 1.6603, Val Acc: 0.2930\n",
      "Epoch 37/100, Loss: 1.6004, Acc: 0.2931, Val Loss: 1.6604, Val Acc: 0.2930\n",
      "Epoch 38/100, Loss: 1.6004, Acc: 0.2931, Val Loss: 1.6604, Val Acc: 0.2930\n",
      "Epoch 39/100, Loss: 1.6004, Acc: 0.2931, Val Loss: 1.6604, Val Acc: 0.2930\n",
      "Epoch 40/100, Loss: 1.6004, Acc: 0.2931, Val Loss: 1.6605, Val Acc: 0.2930\n",
      "Epoch 41/100, Loss: 1.6004, Acc: 0.2931, Val Loss: 1.6605, Val Acc: 0.2930\n",
      "Epoch 42/100, Loss: 1.6004, Acc: 0.2931, Val Loss: 1.6605, Val Acc: 0.2930\n",
      "Epoch 43/100, Loss: 1.6004, Acc: 0.2930, Val Loss: 1.6605, Val Acc: 0.2930\n",
      "Epoch 44/100, Loss: 1.6004, Acc: 0.2929, Val Loss: 1.6606, Val Acc: 0.2930\n",
      "Epoch 45/100, Loss: 1.6004, Acc: 0.2928, Val Loss: 1.6606, Val Acc: 0.2930\n",
      "Epoch 46/100, Loss: 1.6004, Acc: 0.2928, Val Loss: 1.6606, Val Acc: 0.2930\n",
      "Epoch 47/100, Loss: 1.6004, Acc: 0.2928, Val Loss: 1.6606, Val Acc: 0.2930\n",
      "Epoch 48/100, Loss: 1.6004, Acc: 0.2928, Val Loss: 1.6607, Val Acc: 0.2930\n",
      "Epoch 49/100, Loss: 1.6004, Acc: 0.2925, Val Loss: 1.6653, Val Acc: 0.2930\n",
      "Epoch 50/100, Loss: 1.6016, Acc: 0.2923, Val Loss: 1.6461, Val Acc: 0.2930\n",
      "Epoch 51/100, Loss: 1.6009, Acc: 0.2926, Val Loss: 1.6464, Val Acc: 0.2930\n",
      "Epoch 52/100, Loss: 1.6009, Acc: 0.2926, Val Loss: 1.6465, Val Acc: 0.2930\n",
      "Epoch 53/100, Loss: 1.6008, Acc: 0.2926, Val Loss: 1.6506, Val Acc: 0.2930\n",
      "Epoch 54/100, Loss: 1.6014, Acc: 0.2918, Val Loss: 1.6528, Val Acc: 0.2930\n",
      "Epoch 55/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6535, Val Acc: 0.2930\n",
      "Epoch 56/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6540, Val Acc: 0.2930\n",
      "Epoch 57/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6544, Val Acc: 0.2930\n",
      "Epoch 58/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6547, Val Acc: 0.2930\n",
      "Epoch 59/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6550, Val Acc: 0.2930\n",
      "Epoch 60/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6552, Val Acc: 0.2930\n",
      "Epoch 61/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6554, Val Acc: 0.2930\n",
      "Epoch 62/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6556, Val Acc: 0.2930\n",
      "Epoch 63/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6558, Val Acc: 0.2930\n",
      "Epoch 64/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6559, Val Acc: 0.2930\n",
      "Epoch 65/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6561, Val Acc: 0.2930\n",
      "Epoch 66/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6562, Val Acc: 0.2930\n",
      "Epoch 67/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6563, Val Acc: 0.2930\n",
      "Epoch 68/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6564, Val Acc: 0.2930\n",
      "Epoch 69/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6565, Val Acc: 0.2930\n",
      "Epoch 70/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6567, Val Acc: 0.2930\n",
      "Epoch 71/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6568, Val Acc: 0.2930\n",
      "Epoch 72/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6568, Val Acc: 0.2930\n",
      "Epoch 73/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6569, Val Acc: 0.2930\n",
      "Epoch 74/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6570, Val Acc: 0.2930\n",
      "Epoch 75/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6571, Val Acc: 0.2930\n",
      "Epoch 76/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6572, Val Acc: 0.2930\n",
      "Epoch 77/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6572, Val Acc: 0.2930\n",
      "Epoch 78/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6572, Val Acc: 0.2930\n",
      "Epoch 79/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6572, Val Acc: 0.2930\n",
      "Epoch 80/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6572, Val Acc: 0.2930\n",
      "Epoch 81/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6572, Val Acc: 0.2930\n",
      "Epoch 82/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6572, Val Acc: 0.2930\n",
      "Epoch 83/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 84/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 85/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 86/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 87/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 88/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 89/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 90/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 91/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 92/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6573, Val Acc: 0.2930\n",
      "Epoch 93/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "Epoch 94/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "Epoch 95/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "Epoch 96/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "Epoch 97/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "Epoch 98/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "Epoch 99/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "Epoch 100/100, Loss: 1.6008, Acc: 0.2923, Val Loss: 1.6574, Val Acc: 0.2930\n",
      "✅ Accuracy : 0.2930\n",
      "✅ Precision: 0.0419\n",
      "✅ Recall   : 0.1426\n",
      "✅ F1 Score : 0.0648\n",
      "\n",
      "🔁 Running config 6/12\n",
      "Layers: [179, 16, 7], Activations: ['tanh', 'softmax'], LR: 0.1\n",
      "Epoch 1/100, Loss: 1.2684, Acc: 0.4973, Val Loss: 1.0194, Val Acc: 0.5900\n",
      "Epoch 2/100, Loss: 0.9611, Acc: 0.6170, Val Loss: 0.9732, Val Acc: 0.6015\n",
      "Epoch 3/100, Loss: 0.8923, Acc: 0.6435, Val Loss: 0.9445, Val Acc: 0.6385\n",
      "Epoch 4/100, Loss: 0.8192, Acc: 0.6800, Val Loss: 0.7763, Val Acc: 0.6825\n",
      "Epoch 5/100, Loss: 0.7451, Acc: 0.7117, Val Loss: 0.7475, Val Acc: 0.7020\n",
      "Epoch 6/100, Loss: 0.7636, Acc: 0.7074, Val Loss: 0.7468, Val Acc: 0.6995\n",
      "Epoch 7/100, Loss: 0.7259, Acc: 0.7159, Val Loss: 0.7643, Val Acc: 0.7070\n",
      "Epoch 8/100, Loss: 0.7180, Acc: 0.7150, Val Loss: 0.6678, Val Acc: 0.7350\n",
      "Epoch 9/100, Loss: 0.7056, Acc: 0.7295, Val Loss: 0.7040, Val Acc: 0.7250\n",
      "Epoch 10/100, Loss: 0.6821, Acc: 0.7356, Val Loss: 0.7565, Val Acc: 0.7260\n",
      "Epoch 11/100, Loss: 0.6920, Acc: 0.7355, Val Loss: 0.7144, Val Acc: 0.7380\n",
      "Epoch 12/100, Loss: 0.6767, Acc: 0.7418, Val Loss: 0.7481, Val Acc: 0.7170\n",
      "Epoch 13/100, Loss: 0.6552, Acc: 0.7479, Val Loss: 0.7296, Val Acc: 0.7280\n",
      "Epoch 14/100, Loss: 0.6528, Acc: 0.7506, Val Loss: 0.6625, Val Acc: 0.7455\n",
      "Epoch 15/100, Loss: 0.6321, Acc: 0.7595, Val Loss: 0.6988, Val Acc: 0.7370\n",
      "Epoch 16/100, Loss: 0.6188, Acc: 0.7590, Val Loss: 0.7591, Val Acc: 0.7300\n",
      "Epoch 17/100, Loss: 0.6191, Acc: 0.7619, Val Loss: 0.8228, Val Acc: 0.7230\n",
      "Epoch 18/100, Loss: 0.6373, Acc: 0.7565, Val Loss: 0.7368, Val Acc: 0.7220\n",
      "Epoch 19/100, Loss: 0.6337, Acc: 0.7575, Val Loss: 0.6670, Val Acc: 0.7390\n",
      "Epoch 20/100, Loss: 0.6068, Acc: 0.7660, Val Loss: 0.6538, Val Acc: 0.7485\n",
      "Epoch 21/100, Loss: 0.6026, Acc: 0.7704, Val Loss: 0.6816, Val Acc: 0.7555\n",
      "Epoch 22/100, Loss: 0.5956, Acc: 0.7698, Val Loss: 0.7007, Val Acc: 0.7440\n",
      "Epoch 23/100, Loss: 0.5997, Acc: 0.7641, Val Loss: 0.6947, Val Acc: 0.7515\n",
      "Epoch 24/100, Loss: 0.5868, Acc: 0.7704, Val Loss: 0.7122, Val Acc: 0.7470\n",
      "Epoch 25/100, Loss: 0.5941, Acc: 0.7751, Val Loss: 0.6904, Val Acc: 0.7465\n",
      "Epoch 26/100, Loss: 0.5685, Acc: 0.7871, Val Loss: 0.6685, Val Acc: 0.7530\n",
      "Epoch 27/100, Loss: 0.5829, Acc: 0.7824, Val Loss: 0.6595, Val Acc: 0.7685\n",
      "Epoch 28/100, Loss: 0.5760, Acc: 0.7836, Val Loss: 0.7543, Val Acc: 0.7355\n",
      "Epoch 29/100, Loss: 0.5725, Acc: 0.7799, Val Loss: 0.6986, Val Acc: 0.7515\n",
      "Epoch 30/100, Loss: 0.5704, Acc: 0.7851, Val Loss: 0.6541, Val Acc: 0.7565\n",
      "Epoch 31/100, Loss: 0.5465, Acc: 0.7893, Val Loss: 0.7191, Val Acc: 0.7520\n",
      "Epoch 32/100, Loss: 0.5515, Acc: 0.7910, Val Loss: 0.7465, Val Acc: 0.7305\n",
      "Epoch 33/100, Loss: 0.5438, Acc: 0.7904, Val Loss: 0.7260, Val Acc: 0.7365\n",
      "Epoch 34/100, Loss: 0.5338, Acc: 0.7976, Val Loss: 0.6620, Val Acc: 0.7635\n",
      "Epoch 35/100, Loss: 0.5579, Acc: 0.7901, Val Loss: 0.7093, Val Acc: 0.7430\n",
      "Epoch 36/100, Loss: 0.5578, Acc: 0.7903, Val Loss: 0.8254, Val Acc: 0.7195\n",
      "Epoch 37/100, Loss: 0.5422, Acc: 0.7976, Val Loss: 0.6976, Val Acc: 0.7565\n",
      "Epoch 38/100, Loss: 0.5188, Acc: 0.8010, Val Loss: 0.7717, Val Acc: 0.7490\n",
      "Epoch 39/100, Loss: 0.5263, Acc: 0.8056, Val Loss: 0.6692, Val Acc: 0.7635\n",
      "Epoch 40/100, Loss: 0.5283, Acc: 0.8024, Val Loss: 0.7193, Val Acc: 0.7520\n",
      "Epoch 41/100, Loss: 0.5044, Acc: 0.8125, Val Loss: 0.6806, Val Acc: 0.7755\n",
      "Epoch 42/100, Loss: 0.5144, Acc: 0.8117, Val Loss: 0.7192, Val Acc: 0.7555\n",
      "Epoch 43/100, Loss: 0.5361, Acc: 0.7959, Val Loss: 0.7508, Val Acc: 0.7455\n",
      "Epoch 44/100, Loss: 0.5084, Acc: 0.8109, Val Loss: 0.7462, Val Acc: 0.7365\n",
      "Epoch 45/100, Loss: 0.5324, Acc: 0.7941, Val Loss: 0.7596, Val Acc: 0.7475\n",
      "Epoch 46/100, Loss: 0.5202, Acc: 0.8000, Val Loss: 0.8100, Val Acc: 0.7380\n",
      "Epoch 47/100, Loss: 0.5205, Acc: 0.8023, Val Loss: 0.7254, Val Acc: 0.7470\n",
      "Epoch 48/100, Loss: 0.5020, Acc: 0.8091, Val Loss: 0.7293, Val Acc: 0.7505\n",
      "Epoch 49/100, Loss: 0.4998, Acc: 0.8067, Val Loss: 0.8143, Val Acc: 0.7290\n",
      "Epoch 50/100, Loss: 0.5194, Acc: 0.8000, Val Loss: 0.7339, Val Acc: 0.7710\n",
      "Epoch 51/100, Loss: 0.5382, Acc: 0.7971, Val Loss: 0.7052, Val Acc: 0.7745\n",
      "Epoch 52/100, Loss: 0.5089, Acc: 0.8069, Val Loss: 0.7464, Val Acc: 0.7475\n",
      "Epoch 53/100, Loss: 0.5193, Acc: 0.8050, Val Loss: 0.7877, Val Acc: 0.7495\n",
      "Epoch 54/100, Loss: 0.5036, Acc: 0.8090, Val Loss: 0.7833, Val Acc: 0.7340\n",
      "Epoch 55/100, Loss: 0.5005, Acc: 0.8120, Val Loss: 0.7951, Val Acc: 0.7415\n",
      "Epoch 56/100, Loss: 0.4928, Acc: 0.8120, Val Loss: 0.6381, Val Acc: 0.7800\n",
      "Epoch 57/100, Loss: 0.4957, Acc: 0.8156, Val Loss: 0.6485, Val Acc: 0.7845\n",
      "Epoch 58/100, Loss: 0.4713, Acc: 0.8216, Val Loss: 0.6198, Val Acc: 0.7830\n",
      "Epoch 59/100, Loss: 0.4668, Acc: 0.8273, Val Loss: 0.6205, Val Acc: 0.7755\n",
      "Epoch 60/100, Loss: 0.4757, Acc: 0.8257, Val Loss: 0.6278, Val Acc: 0.7715\n",
      "Epoch 61/100, Loss: 0.4841, Acc: 0.8225, Val Loss: 0.6388, Val Acc: 0.7800\n",
      "Epoch 62/100, Loss: 0.4700, Acc: 0.8259, Val Loss: 0.6408, Val Acc: 0.7865\n",
      "Epoch 63/100, Loss: 0.4718, Acc: 0.8251, Val Loss: 0.6144, Val Acc: 0.7735\n",
      "Epoch 64/100, Loss: 0.4616, Acc: 0.8256, Val Loss: 0.6643, Val Acc: 0.7800\n",
      "Epoch 65/100, Loss: 0.4712, Acc: 0.8213, Val Loss: 0.6602, Val Acc: 0.7840\n",
      "Epoch 66/100, Loss: 0.4621, Acc: 0.8291, Val Loss: 0.7379, Val Acc: 0.7475\n",
      "Epoch 67/100, Loss: 0.4567, Acc: 0.8297, Val Loss: 0.6624, Val Acc: 0.7770\n",
      "Epoch 68/100, Loss: 0.4444, Acc: 0.8373, Val Loss: 0.6346, Val Acc: 0.7935\n",
      "Epoch 69/100, Loss: 0.4593, Acc: 0.8289, Val Loss: 0.6420, Val Acc: 0.7925\n",
      "Epoch 70/100, Loss: 0.4472, Acc: 0.8295, Val Loss: 0.6917, Val Acc: 0.7770\n",
      "Epoch 71/100, Loss: 0.4576, Acc: 0.8303, Val Loss: 0.6951, Val Acc: 0.7625\n",
      "Epoch 72/100, Loss: 0.4402, Acc: 0.8331, Val Loss: 0.6294, Val Acc: 0.7890\n",
      "Epoch 73/100, Loss: 0.4486, Acc: 0.8345, Val Loss: 0.6845, Val Acc: 0.7675\n",
      "Epoch 74/100, Loss: 0.4421, Acc: 0.8339, Val Loss: 0.6802, Val Acc: 0.7790\n",
      "Epoch 75/100, Loss: 0.4478, Acc: 0.8331, Val Loss: 0.7121, Val Acc: 0.7680\n",
      "Epoch 76/100, Loss: 0.4553, Acc: 0.8296, Val Loss: 0.6900, Val Acc: 0.7810\n",
      "Epoch 77/100, Loss: 0.4444, Acc: 0.8344, Val Loss: 0.7109, Val Acc: 0.7705\n",
      "Epoch 78/100, Loss: 0.4503, Acc: 0.8314, Val Loss: 0.6964, Val Acc: 0.7900\n",
      "Epoch 79/100, Loss: 0.4377, Acc: 0.8381, Val Loss: 0.6278, Val Acc: 0.7905\n",
      "Epoch 80/100, Loss: 0.4175, Acc: 0.8446, Val Loss: 0.6334, Val Acc: 0.7880\n",
      "Epoch 81/100, Loss: 0.4368, Acc: 0.8385, Val Loss: 0.6177, Val Acc: 0.8000\n",
      "Epoch 82/100, Loss: 0.4222, Acc: 0.8399, Val Loss: 0.6043, Val Acc: 0.8025\n",
      "Epoch 83/100, Loss: 0.4266, Acc: 0.8426, Val Loss: 0.6762, Val Acc: 0.7765\n",
      "Epoch 84/100, Loss: 0.4145, Acc: 0.8445, Val Loss: 0.7186, Val Acc: 0.7760\n",
      "Epoch 85/100, Loss: 0.4189, Acc: 0.8409, Val Loss: 0.6603, Val Acc: 0.7890\n",
      "Epoch 86/100, Loss: 0.4128, Acc: 0.8472, Val Loss: 0.6324, Val Acc: 0.7840\n",
      "Epoch 87/100, Loss: 0.4033, Acc: 0.8541, Val Loss: 0.7161, Val Acc: 0.7755\n",
      "Epoch 88/100, Loss: 0.4194, Acc: 0.8510, Val Loss: 0.6875, Val Acc: 0.7810\n",
      "Epoch 89/100, Loss: 0.4017, Acc: 0.8562, Val Loss: 0.6232, Val Acc: 0.7975\n",
      "Epoch 90/100, Loss: 0.4067, Acc: 0.8485, Val Loss: 0.6538, Val Acc: 0.7960\n",
      "Epoch 91/100, Loss: 0.4105, Acc: 0.8504, Val Loss: 0.6981, Val Acc: 0.7975\n",
      "Epoch 92/100, Loss: 0.3908, Acc: 0.8584, Val Loss: 0.6338, Val Acc: 0.8020\n",
      "Epoch 93/100, Loss: 0.4228, Acc: 0.8449, Val Loss: 0.6694, Val Acc: 0.7820\n",
      "Epoch 94/100, Loss: 0.4049, Acc: 0.8506, Val Loss: 0.6696, Val Acc: 0.7960\n",
      "Epoch 95/100, Loss: 0.4149, Acc: 0.8468, Val Loss: 0.6819, Val Acc: 0.7870\n",
      "Epoch 96/100, Loss: 0.4097, Acc: 0.8479, Val Loss: 0.7656, Val Acc: 0.7865\n",
      "Epoch 97/100, Loss: 0.4044, Acc: 0.8499, Val Loss: 0.6585, Val Acc: 0.7940\n",
      "Epoch 98/100, Loss: 0.4020, Acc: 0.8576, Val Loss: 0.6927, Val Acc: 0.7825\n",
      "Epoch 99/100, Loss: 0.3926, Acc: 0.8580, Val Loss: 0.7083, Val Acc: 0.7815\n",
      "Epoch 100/100, Loss: 0.4013, Acc: 0.8526, Val Loss: 0.7290, Val Acc: 0.7900\n",
      "✅ Accuracy : 0.7900\n",
      "✅ Precision: 0.6237\n",
      "✅ Recall   : 0.6465\n",
      "✅ F1 Score : 0.6229\n",
      "\n",
      "🔁 Running config 7/12\n",
      "Layers: [179, 16, 8, 7], Activations: ['relu', 'relu', 'softmax'], LR: 0.01\n",
      "Epoch 1/100, Loss: 1.6193, Acc: 0.3680, Val Loss: 1.3036, Val Acc: 0.4520\n",
      "Epoch 2/100, Loss: 1.2110, Acc: 0.4861, Val Loss: 1.0415, Val Acc: 0.5740\n",
      "Epoch 3/100, Loss: 0.9565, Acc: 0.5991, Val Loss: 0.9555, Val Acc: 0.6430\n",
      "Epoch 4/100, Loss: 0.8832, Acc: 0.6496, Val Loss: 0.7614, Val Acc: 0.6975\n",
      "Epoch 5/100, Loss: 0.8278, Acc: 0.6850, Val Loss: 0.6713, Val Acc: 0.7405\n",
      "Epoch 6/100, Loss: 0.6873, Acc: 0.7345, Val Loss: 0.5897, Val Acc: 0.7790\n",
      "Epoch 7/100, Loss: 0.6371, Acc: 0.7559, Val Loss: 0.5972, Val Acc: 0.7750\n",
      "Epoch 8/100, Loss: 0.5957, Acc: 0.7714, Val Loss: 0.5727, Val Acc: 0.7915\n",
      "Epoch 9/100, Loss: 0.5728, Acc: 0.7844, Val Loss: 0.5672, Val Acc: 0.7840\n",
      "Epoch 10/100, Loss: 0.5689, Acc: 0.7856, Val Loss: 0.5589, Val Acc: 0.7850\n",
      "Epoch 11/100, Loss: 0.5750, Acc: 0.7893, Val Loss: 0.5280, Val Acc: 0.7985\n",
      "Epoch 12/100, Loss: 0.5247, Acc: 0.7989, Val Loss: 0.5509, Val Acc: 0.7890\n",
      "Epoch 13/100, Loss: 0.5085, Acc: 0.8065, Val Loss: 0.5360, Val Acc: 0.7855\n",
      "Epoch 14/100, Loss: 0.5067, Acc: 0.8069, Val Loss: 0.5177, Val Acc: 0.7990\n",
      "Epoch 15/100, Loss: 0.4934, Acc: 0.8134, Val Loss: 0.5416, Val Acc: 0.7845\n",
      "Epoch 16/100, Loss: 0.4839, Acc: 0.8159, Val Loss: 0.5292, Val Acc: 0.7920\n",
      "Epoch 17/100, Loss: 0.4791, Acc: 0.8201, Val Loss: 0.5369, Val Acc: 0.7890\n",
      "Epoch 18/100, Loss: 0.4708, Acc: 0.8246, Val Loss: 0.5192, Val Acc: 0.7945\n",
      "Epoch 19/100, Loss: 0.4689, Acc: 0.8269, Val Loss: 0.5424, Val Acc: 0.7890\n",
      "Epoch 20/100, Loss: 0.4699, Acc: 0.8256, Val Loss: 0.4877, Val Acc: 0.8095\n",
      "Epoch 21/100, Loss: 0.4645, Acc: 0.8267, Val Loss: 0.5553, Val Acc: 0.7875\n",
      "Epoch 22/100, Loss: 0.4459, Acc: 0.8325, Val Loss: 0.5355, Val Acc: 0.7895\n",
      "Epoch 23/100, Loss: 0.4473, Acc: 0.8317, Val Loss: 0.5378, Val Acc: 0.7930\n",
      "Epoch 24/100, Loss: 0.4418, Acc: 0.8349, Val Loss: 0.5497, Val Acc: 0.7940\n",
      "Epoch 25/100, Loss: 0.4369, Acc: 0.8353, Val Loss: 0.5280, Val Acc: 0.8010\n",
      "Epoch 26/100, Loss: 0.4694, Acc: 0.8324, Val Loss: 0.5185, Val Acc: 0.7965\n",
      "Epoch 27/100, Loss: 0.4275, Acc: 0.8409, Val Loss: 0.5133, Val Acc: 0.7970\n",
      "Epoch 28/100, Loss: 0.4244, Acc: 0.8401, Val Loss: 0.4778, Val Acc: 0.8205\n",
      "Epoch 29/100, Loss: 0.4142, Acc: 0.8456, Val Loss: 0.4475, Val Acc: 0.8255\n",
      "Epoch 30/100, Loss: 0.4006, Acc: 0.8525, Val Loss: 0.4781, Val Acc: 0.8145\n",
      "Epoch 31/100, Loss: 0.4002, Acc: 0.8544, Val Loss: 0.5339, Val Acc: 0.8060\n",
      "Epoch 32/100, Loss: 0.4007, Acc: 0.8515, Val Loss: 0.4664, Val Acc: 0.8140\n",
      "Epoch 33/100, Loss: 0.4136, Acc: 0.8558, Val Loss: 0.4547, Val Acc: 0.8165\n",
      "Epoch 34/100, Loss: 0.4022, Acc: 0.8570, Val Loss: 0.4293, Val Acc: 0.8385\n",
      "Epoch 35/100, Loss: 0.3885, Acc: 0.8634, Val Loss: 0.4539, Val Acc: 0.8270\n",
      "Epoch 36/100, Loss: 0.3855, Acc: 0.8596, Val Loss: 0.4538, Val Acc: 0.8290\n",
      "Epoch 37/100, Loss: 0.3884, Acc: 0.8601, Val Loss: 0.4918, Val Acc: 0.8210\n",
      "Epoch 38/100, Loss: 0.3905, Acc: 0.8564, Val Loss: 0.4138, Val Acc: 0.8460\n",
      "Epoch 39/100, Loss: 0.3727, Acc: 0.8648, Val Loss: 0.4503, Val Acc: 0.8385\n",
      "Epoch 40/100, Loss: 0.3805, Acc: 0.8662, Val Loss: 0.4516, Val Acc: 0.8405\n",
      "Epoch 41/100, Loss: 0.3614, Acc: 0.8701, Val Loss: 0.4666, Val Acc: 0.8315\n",
      "Epoch 42/100, Loss: 0.3641, Acc: 0.8688, Val Loss: 0.4680, Val Acc: 0.8295\n",
      "Epoch 43/100, Loss: 0.3652, Acc: 0.8670, Val Loss: 0.4557, Val Acc: 0.8350\n",
      "Epoch 44/100, Loss: 0.3789, Acc: 0.8665, Val Loss: 0.5313, Val Acc: 0.8065\n",
      "Epoch 45/100, Loss: 0.3516, Acc: 0.8742, Val Loss: 0.5128, Val Acc: 0.8165\n",
      "Epoch 46/100, Loss: 0.3653, Acc: 0.8660, Val Loss: 0.5340, Val Acc: 0.8055\n",
      "Epoch 47/100, Loss: 0.3569, Acc: 0.8704, Val Loss: 0.5410, Val Acc: 0.8085\n",
      "Epoch 48/100, Loss: 0.3449, Acc: 0.8734, Val Loss: 0.4739, Val Acc: 0.8245\n",
      "Epoch 49/100, Loss: 0.3513, Acc: 0.8710, Val Loss: 0.4951, Val Acc: 0.8310\n",
      "Epoch 50/100, Loss: 0.3751, Acc: 0.8695, Val Loss: 0.4894, Val Acc: 0.8295\n",
      "Epoch 51/100, Loss: 0.3865, Acc: 0.8639, Val Loss: 0.4889, Val Acc: 0.8315\n",
      "Epoch 52/100, Loss: 0.3662, Acc: 0.8676, Val Loss: 0.4474, Val Acc: 0.8380\n",
      "Epoch 53/100, Loss: 0.3490, Acc: 0.8754, Val Loss: 0.5049, Val Acc: 0.8265\n",
      "Epoch 54/100, Loss: 0.3616, Acc: 0.8706, Val Loss: 0.5162, Val Acc: 0.8310\n",
      "Epoch 55/100, Loss: 0.3491, Acc: 0.8742, Val Loss: 0.5260, Val Acc: 0.8190\n",
      "Epoch 56/100, Loss: 0.3505, Acc: 0.8741, Val Loss: 0.5914, Val Acc: 0.7965\n",
      "Epoch 57/100, Loss: 0.3833, Acc: 0.8718, Val Loss: 0.5537, Val Acc: 0.8195\n",
      "Epoch 58/100, Loss: 0.3586, Acc: 0.8719, Val Loss: 0.5387, Val Acc: 0.8155\n",
      "Epoch 59/100, Loss: 0.3515, Acc: 0.8775, Val Loss: 0.5029, Val Acc: 0.8270\n",
      "Epoch 60/100, Loss: 0.3606, Acc: 0.8735, Val Loss: 0.5951, Val Acc: 0.8000\n",
      "Epoch 61/100, Loss: 0.3553, Acc: 0.8709, Val Loss: 0.5917, Val Acc: 0.8100\n",
      "Epoch 62/100, Loss: 0.3506, Acc: 0.8759, Val Loss: 0.5704, Val Acc: 0.8185\n",
      "Epoch 63/100, Loss: 0.3667, Acc: 0.8676, Val Loss: 0.5496, Val Acc: 0.8210\n",
      "Epoch 64/100, Loss: 0.3538, Acc: 0.8740, Val Loss: 0.6532, Val Acc: 0.8020\n",
      "Epoch 65/100, Loss: 0.3499, Acc: 0.8768, Val Loss: 0.5560, Val Acc: 0.8195\n",
      "Epoch 66/100, Loss: 0.3411, Acc: 0.8760, Val Loss: 0.5787, Val Acc: 0.8150\n",
      "Epoch 67/100, Loss: 0.3324, Acc: 0.8792, Val Loss: 0.6214, Val Acc: 0.8005\n",
      "Epoch 68/100, Loss: 0.3327, Acc: 0.8786, Val Loss: 0.5925, Val Acc: 0.8115\n",
      "Epoch 69/100, Loss: 0.3499, Acc: 0.8730, Val Loss: 0.5807, Val Acc: 0.8230\n",
      "Epoch 70/100, Loss: 0.3516, Acc: 0.8776, Val Loss: 0.5712, Val Acc: 0.8190\n",
      "Epoch 71/100, Loss: 0.3447, Acc: 0.8790, Val Loss: 0.6089, Val Acc: 0.8075\n",
      "Epoch 72/100, Loss: 0.3265, Acc: 0.8859, Val Loss: 0.5710, Val Acc: 0.8200\n",
      "Epoch 73/100, Loss: 0.3177, Acc: 0.8858, Val Loss: 0.7542, Val Acc: 0.7920\n",
      "Epoch 74/100, Loss: 0.3265, Acc: 0.8849, Val Loss: 0.7430, Val Acc: 0.7810\n",
      "Epoch 75/100, Loss: 0.3344, Acc: 0.8822, Val Loss: 0.6090, Val Acc: 0.8030\n",
      "Epoch 76/100, Loss: 0.3317, Acc: 0.8825, Val Loss: 0.6239, Val Acc: 0.8070\n",
      "Epoch 77/100, Loss: 0.3358, Acc: 0.8779, Val Loss: 0.5919, Val Acc: 0.8120\n",
      "Epoch 78/100, Loss: 0.3407, Acc: 0.8800, Val Loss: 0.5962, Val Acc: 0.8085\n",
      "Epoch 79/100, Loss: 0.3376, Acc: 0.8798, Val Loss: 0.6111, Val Acc: 0.8100\n",
      "Epoch 80/100, Loss: 0.3260, Acc: 0.8852, Val Loss: 0.5969, Val Acc: 0.8160\n",
      "Epoch 81/100, Loss: 0.3210, Acc: 0.8855, Val Loss: 0.5793, Val Acc: 0.8210\n",
      "Epoch 82/100, Loss: 0.3321, Acc: 0.8839, Val Loss: 0.5953, Val Acc: 0.8265\n",
      "Epoch 83/100, Loss: 0.3364, Acc: 0.8804, Val Loss: 0.5815, Val Acc: 0.8200\n",
      "Epoch 84/100, Loss: 0.3301, Acc: 0.8802, Val Loss: 0.6737, Val Acc: 0.7970\n",
      "Epoch 85/100, Loss: 0.3282, Acc: 0.8835, Val Loss: 0.7212, Val Acc: 0.7865\n",
      "Epoch 86/100, Loss: 0.3142, Acc: 0.8885, Val Loss: 0.6365, Val Acc: 0.8085\n",
      "Epoch 87/100, Loss: 0.3205, Acc: 0.8856, Val Loss: 0.6601, Val Acc: 0.8080\n",
      "Epoch 88/100, Loss: 0.3188, Acc: 0.8862, Val Loss: 0.6114, Val Acc: 0.8215\n",
      "Epoch 89/100, Loss: 0.3314, Acc: 0.8851, Val Loss: 0.6629, Val Acc: 0.8035\n",
      "Epoch 90/100, Loss: 0.3242, Acc: 0.8860, Val Loss: 0.5781, Val Acc: 0.8305\n",
      "Epoch 91/100, Loss: 0.3162, Acc: 0.8874, Val Loss: 0.6656, Val Acc: 0.7955\n",
      "Epoch 92/100, Loss: 0.3400, Acc: 0.8811, Val Loss: 0.5696, Val Acc: 0.8285\n",
      "Epoch 93/100, Loss: 0.3245, Acc: 0.8858, Val Loss: 0.7021, Val Acc: 0.7820\n",
      "Epoch 94/100, Loss: 0.3235, Acc: 0.8850, Val Loss: 0.5967, Val Acc: 0.8070\n",
      "Epoch 95/100, Loss: 0.3162, Acc: 0.8900, Val Loss: 0.6040, Val Acc: 0.8120\n",
      "Epoch 96/100, Loss: 0.3216, Acc: 0.8878, Val Loss: 0.6633, Val Acc: 0.7915\n",
      "Epoch 97/100, Loss: 0.3166, Acc: 0.8888, Val Loss: 0.5470, Val Acc: 0.8315\n",
      "Epoch 98/100, Loss: 0.3260, Acc: 0.8859, Val Loss: 0.5591, Val Acc: 0.8125\n",
      "Epoch 99/100, Loss: 0.3503, Acc: 0.8806, Val Loss: 0.6549, Val Acc: 0.7885\n",
      "Epoch 100/100, Loss: 0.3470, Acc: 0.8786, Val Loss: 0.6835, Val Acc: 0.7810\n",
      "✅ Accuracy : 0.7810\n",
      "✅ Precision: 0.6042\n",
      "✅ Recall   : 0.5375\n",
      "✅ F1 Score : 0.5555\n",
      "\n",
      "🔁 Running config 8/12\n",
      "Layers: [179, 16, 8, 7], Activations: ['tanh', 'relu', 'softmax'], LR: 0.01\n",
      "Epoch 1/100, Loss: 1.6269, Acc: 0.3088, Val Loss: 1.3862, Val Acc: 0.3720\n",
      "Epoch 2/100, Loss: 1.2022, Acc: 0.4559, Val Loss: 1.1091, Val Acc: 0.5105\n",
      "Epoch 3/100, Loss: 0.9990, Acc: 0.5505, Val Loss: 0.9708, Val Acc: 0.5610\n",
      "Epoch 4/100, Loss: 0.8686, Acc: 0.6166, Val Loss: 0.8799, Val Acc: 0.6010\n",
      "Epoch 5/100, Loss: 0.7738, Acc: 0.6564, Val Loss: 0.8511, Val Acc: 0.6270\n",
      "Epoch 6/100, Loss: 0.7200, Acc: 0.6857, Val Loss: 0.8241, Val Acc: 0.6500\n",
      "Epoch 7/100, Loss: 0.6831, Acc: 0.7040, Val Loss: 0.8118, Val Acc: 0.6545\n",
      "Epoch 8/100, Loss: 0.6509, Acc: 0.7194, Val Loss: 0.7713, Val Acc: 0.6745\n",
      "Epoch 9/100, Loss: 0.6254, Acc: 0.7318, Val Loss: 0.7486, Val Acc: 0.6795\n",
      "Epoch 10/100, Loss: 0.5998, Acc: 0.7448, Val Loss: 0.7235, Val Acc: 0.6935\n",
      "Epoch 11/100, Loss: 0.5801, Acc: 0.7548, Val Loss: 0.7156, Val Acc: 0.6965\n",
      "Epoch 12/100, Loss: 0.5572, Acc: 0.7652, Val Loss: 0.7090, Val Acc: 0.6980\n",
      "Epoch 13/100, Loss: 0.5456, Acc: 0.7719, Val Loss: 0.7251, Val Acc: 0.7065\n",
      "Epoch 14/100, Loss: 0.5287, Acc: 0.7788, Val Loss: 0.7198, Val Acc: 0.7015\n",
      "Epoch 15/100, Loss: 0.5213, Acc: 0.7830, Val Loss: 0.7259, Val Acc: 0.6975\n",
      "Epoch 16/100, Loss: 0.5190, Acc: 0.7814, Val Loss: 0.7330, Val Acc: 0.6985\n",
      "Epoch 17/100, Loss: 0.5026, Acc: 0.7894, Val Loss: 0.7047, Val Acc: 0.7010\n",
      "Epoch 18/100, Loss: 0.4930, Acc: 0.8004, Val Loss: 0.7057, Val Acc: 0.7030\n",
      "Epoch 19/100, Loss: 0.4894, Acc: 0.7944, Val Loss: 0.7420, Val Acc: 0.6780\n",
      "Epoch 20/100, Loss: 0.4763, Acc: 0.7986, Val Loss: 0.7396, Val Acc: 0.6830\n",
      "Epoch 21/100, Loss: 0.4612, Acc: 0.8071, Val Loss: 0.7072, Val Acc: 0.6830\n",
      "Epoch 22/100, Loss: 0.4521, Acc: 0.8066, Val Loss: 0.7189, Val Acc: 0.6910\n",
      "Epoch 23/100, Loss: 0.4509, Acc: 0.8103, Val Loss: 0.6971, Val Acc: 0.7165\n",
      "Epoch 24/100, Loss: 0.4395, Acc: 0.8144, Val Loss: 0.7098, Val Acc: 0.7070\n",
      "Epoch 25/100, Loss: 0.4331, Acc: 0.8201, Val Loss: 0.7081, Val Acc: 0.7130\n",
      "Epoch 26/100, Loss: 0.4305, Acc: 0.8170, Val Loss: 0.7155, Val Acc: 0.7030\n",
      "Epoch 27/100, Loss: 0.4224, Acc: 0.8205, Val Loss: 0.6882, Val Acc: 0.7220\n",
      "Epoch 28/100, Loss: 0.4176, Acc: 0.8240, Val Loss: 0.6896, Val Acc: 0.7240\n",
      "Epoch 29/100, Loss: 0.4095, Acc: 0.8275, Val Loss: 0.7031, Val Acc: 0.7205\n",
      "Epoch 30/100, Loss: 0.4173, Acc: 0.8265, Val Loss: 0.7260, Val Acc: 0.7095\n",
      "Epoch 31/100, Loss: 0.4127, Acc: 0.8305, Val Loss: 0.6952, Val Acc: 0.7390\n",
      "Epoch 32/100, Loss: 0.4012, Acc: 0.8309, Val Loss: 0.7057, Val Acc: 0.7285\n",
      "Epoch 33/100, Loss: 0.3943, Acc: 0.8360, Val Loss: 0.7034, Val Acc: 0.7325\n",
      "Epoch 34/100, Loss: 0.3883, Acc: 0.8381, Val Loss: 0.7110, Val Acc: 0.7325\n",
      "Epoch 35/100, Loss: 0.3889, Acc: 0.8386, Val Loss: 0.7034, Val Acc: 0.7325\n",
      "Epoch 36/100, Loss: 0.3843, Acc: 0.8424, Val Loss: 0.7119, Val Acc: 0.7365\n",
      "Epoch 37/100, Loss: 0.3787, Acc: 0.8468, Val Loss: 0.7083, Val Acc: 0.7390\n",
      "Epoch 38/100, Loss: 0.3720, Acc: 0.8470, Val Loss: 0.7243, Val Acc: 0.7350\n",
      "Epoch 39/100, Loss: 0.3669, Acc: 0.8498, Val Loss: 0.7224, Val Acc: 0.7395\n",
      "Epoch 40/100, Loss: 0.3657, Acc: 0.8528, Val Loss: 0.7239, Val Acc: 0.7330\n",
      "Epoch 41/100, Loss: 0.3610, Acc: 0.8544, Val Loss: 0.7206, Val Acc: 0.7380\n",
      "Epoch 42/100, Loss: 0.3561, Acc: 0.8576, Val Loss: 0.7472, Val Acc: 0.7355\n",
      "Epoch 43/100, Loss: 0.3542, Acc: 0.8595, Val Loss: 0.7365, Val Acc: 0.7315\n",
      "Epoch 44/100, Loss: 0.3564, Acc: 0.8536, Val Loss: 0.7056, Val Acc: 0.7390\n",
      "Epoch 45/100, Loss: 0.3561, Acc: 0.8571, Val Loss: 0.7247, Val Acc: 0.7420\n",
      "Epoch 46/100, Loss: 0.3516, Acc: 0.8614, Val Loss: 0.7473, Val Acc: 0.7345\n",
      "Epoch 47/100, Loss: 0.3453, Acc: 0.8608, Val Loss: 0.7511, Val Acc: 0.7330\n",
      "Epoch 48/100, Loss: 0.3405, Acc: 0.8652, Val Loss: 0.7697, Val Acc: 0.7335\n",
      "Epoch 49/100, Loss: 0.3380, Acc: 0.8644, Val Loss: 0.7382, Val Acc: 0.7350\n",
      "Epoch 50/100, Loss: 0.3408, Acc: 0.8606, Val Loss: 0.7357, Val Acc: 0.7430\n",
      "Epoch 51/100, Loss: 0.3363, Acc: 0.8649, Val Loss: 0.7223, Val Acc: 0.7440\n",
      "Epoch 52/100, Loss: 0.3302, Acc: 0.8682, Val Loss: 0.7444, Val Acc: 0.7415\n",
      "Epoch 53/100, Loss: 0.3308, Acc: 0.8665, Val Loss: 0.7365, Val Acc: 0.7380\n",
      "Epoch 54/100, Loss: 0.3285, Acc: 0.8680, Val Loss: 0.7373, Val Acc: 0.7415\n",
      "Epoch 55/100, Loss: 0.3327, Acc: 0.8666, Val Loss: 0.7290, Val Acc: 0.7450\n",
      "Epoch 56/100, Loss: 0.3273, Acc: 0.8698, Val Loss: 0.7360, Val Acc: 0.7425\n",
      "Epoch 57/100, Loss: 0.3205, Acc: 0.8721, Val Loss: 0.7240, Val Acc: 0.7450\n",
      "Epoch 58/100, Loss: 0.3213, Acc: 0.8699, Val Loss: 0.7334, Val Acc: 0.7495\n",
      "Epoch 59/100, Loss: 0.3188, Acc: 0.8736, Val Loss: 0.7301, Val Acc: 0.7465\n",
      "Epoch 60/100, Loss: 0.3188, Acc: 0.8724, Val Loss: 0.7479, Val Acc: 0.7420\n",
      "Epoch 61/100, Loss: 0.3097, Acc: 0.8761, Val Loss: 0.7556, Val Acc: 0.7460\n",
      "Epoch 62/100, Loss: 0.3158, Acc: 0.8718, Val Loss: 0.7704, Val Acc: 0.7420\n",
      "Epoch 63/100, Loss: 0.3164, Acc: 0.8735, Val Loss: 0.7620, Val Acc: 0.7400\n",
      "Epoch 64/100, Loss: 0.3094, Acc: 0.8789, Val Loss: 0.7557, Val Acc: 0.7490\n",
      "Epoch 65/100, Loss: 0.3176, Acc: 0.8730, Val Loss: 0.7274, Val Acc: 0.7600\n",
      "Epoch 66/100, Loss: 0.3055, Acc: 0.8771, Val Loss: 0.7230, Val Acc: 0.7535\n",
      "Epoch 67/100, Loss: 0.3027, Acc: 0.8822, Val Loss: 0.7591, Val Acc: 0.7505\n",
      "Epoch 68/100, Loss: 0.2988, Acc: 0.8791, Val Loss: 0.7376, Val Acc: 0.7475\n",
      "Epoch 69/100, Loss: 0.3123, Acc: 0.8791, Val Loss: 0.7158, Val Acc: 0.7590\n",
      "Epoch 70/100, Loss: 0.3034, Acc: 0.8799, Val Loss: 0.7191, Val Acc: 0.7630\n",
      "Epoch 71/100, Loss: 0.2938, Acc: 0.8872, Val Loss: 0.7515, Val Acc: 0.7560\n",
      "Epoch 72/100, Loss: 0.2972, Acc: 0.8815, Val Loss: 0.7637, Val Acc: 0.7520\n",
      "Epoch 73/100, Loss: 0.2969, Acc: 0.8812, Val Loss: 0.7444, Val Acc: 0.7535\n",
      "Epoch 74/100, Loss: 0.2825, Acc: 0.8865, Val Loss: 0.7416, Val Acc: 0.7630\n",
      "Epoch 75/100, Loss: 0.2833, Acc: 0.8876, Val Loss: 0.7402, Val Acc: 0.7570\n",
      "Epoch 76/100, Loss: 0.2992, Acc: 0.8822, Val Loss: 0.7303, Val Acc: 0.7535\n",
      "Epoch 77/100, Loss: 0.2941, Acc: 0.8839, Val Loss: 0.7299, Val Acc: 0.7620\n",
      "Epoch 78/100, Loss: 0.2919, Acc: 0.8849, Val Loss: 0.7266, Val Acc: 0.7575\n",
      "Epoch 79/100, Loss: 0.2884, Acc: 0.8871, Val Loss: 0.7171, Val Acc: 0.7680\n",
      "Epoch 80/100, Loss: 0.2816, Acc: 0.8879, Val Loss: 0.7172, Val Acc: 0.7645\n",
      "Epoch 81/100, Loss: 0.2792, Acc: 0.8900, Val Loss: 0.7237, Val Acc: 0.7695\n",
      "Epoch 82/100, Loss: 0.2727, Acc: 0.8945, Val Loss: 0.7237, Val Acc: 0.7690\n",
      "Epoch 83/100, Loss: 0.2661, Acc: 0.8979, Val Loss: 0.7227, Val Acc: 0.7710\n",
      "Epoch 84/100, Loss: 0.2770, Acc: 0.8924, Val Loss: 0.7119, Val Acc: 0.7715\n",
      "Epoch 85/100, Loss: 0.2774, Acc: 0.8901, Val Loss: 0.6866, Val Acc: 0.7720\n",
      "Epoch 86/100, Loss: 0.2682, Acc: 0.8974, Val Loss: 0.6989, Val Acc: 0.7745\n",
      "Epoch 87/100, Loss: 0.2716, Acc: 0.8984, Val Loss: 0.6700, Val Acc: 0.7790\n",
      "Epoch 88/100, Loss: 0.2571, Acc: 0.9000, Val Loss: 0.7271, Val Acc: 0.7665\n",
      "Epoch 89/100, Loss: 0.2553, Acc: 0.9009, Val Loss: 0.7169, Val Acc: 0.7760\n",
      "Epoch 90/100, Loss: 0.2577, Acc: 0.9009, Val Loss: 0.7225, Val Acc: 0.7670\n",
      "Epoch 91/100, Loss: 0.2702, Acc: 0.8969, Val Loss: 0.6937, Val Acc: 0.7670\n",
      "Epoch 92/100, Loss: 0.2677, Acc: 0.8968, Val Loss: 0.7061, Val Acc: 0.7845\n",
      "Epoch 93/100, Loss: 0.2591, Acc: 0.9025, Val Loss: 0.7343, Val Acc: 0.7685\n",
      "Epoch 94/100, Loss: 0.2542, Acc: 0.9010, Val Loss: 0.7002, Val Acc: 0.7775\n",
      "Epoch 95/100, Loss: 0.2547, Acc: 0.9035, Val Loss: 0.7009, Val Acc: 0.7800\n",
      "Epoch 96/100, Loss: 0.2597, Acc: 0.9034, Val Loss: 0.7168, Val Acc: 0.7755\n",
      "Epoch 97/100, Loss: 0.2471, Acc: 0.9062, Val Loss: 0.7108, Val Acc: 0.7820\n",
      "Epoch 98/100, Loss: 0.2473, Acc: 0.9058, Val Loss: 0.6983, Val Acc: 0.7810\n",
      "Epoch 99/100, Loss: 0.2549, Acc: 0.9021, Val Loss: 0.7126, Val Acc: 0.7855\n",
      "Epoch 100/100, Loss: 0.2512, Acc: 0.9029, Val Loss: 0.7445, Val Acc: 0.7820\n",
      "✅ Accuracy : 0.7820\n",
      "✅ Precision: 0.5744\n",
      "✅ Recall   : 0.6071\n",
      "✅ F1 Score : 0.5771\n",
      "\n",
      "🔁 Running config 9/12\n",
      "Layers: [179, 16, 8, 7], Activations: ['tanh', 'tanh', 'softmax'], LR: 0.01\n",
      "Epoch 1/100, Loss: 1.5738, Acc: 0.3495, Val Loss: 1.3373, Val Acc: 0.4155\n",
      "Epoch 2/100, Loss: 1.2319, Acc: 0.4426, Val Loss: 1.1752, Val Acc: 0.4725\n",
      "Epoch 3/100, Loss: 1.0761, Acc: 0.5140, Val Loss: 1.0550, Val Acc: 0.5265\n",
      "Epoch 4/100, Loss: 0.9696, Acc: 0.5689, Val Loss: 0.9755, Val Acc: 0.5555\n",
      "Epoch 5/100, Loss: 0.9002, Acc: 0.6026, Val Loss: 0.9303, Val Acc: 0.5800\n",
      "Epoch 6/100, Loss: 0.8469, Acc: 0.6320, Val Loss: 0.9024, Val Acc: 0.5995\n",
      "Epoch 7/100, Loss: 0.8018, Acc: 0.6558, Val Loss: 0.8610, Val Acc: 0.6115\n",
      "Epoch 8/100, Loss: 0.7629, Acc: 0.6713, Val Loss: 0.8431, Val Acc: 0.6200\n",
      "Epoch 9/100, Loss: 0.7307, Acc: 0.6893, Val Loss: 0.8316, Val Acc: 0.6315\n",
      "Epoch 10/100, Loss: 0.7110, Acc: 0.7011, Val Loss: 0.8122, Val Acc: 0.6450\n",
      "Epoch 11/100, Loss: 0.6888, Acc: 0.7097, Val Loss: 0.7889, Val Acc: 0.6640\n",
      "Epoch 12/100, Loss: 0.6694, Acc: 0.7214, Val Loss: 0.7798, Val Acc: 0.6710\n",
      "Epoch 13/100, Loss: 0.6486, Acc: 0.7258, Val Loss: 0.7541, Val Acc: 0.6770\n",
      "Epoch 14/100, Loss: 0.6359, Acc: 0.7365, Val Loss: 0.7668, Val Acc: 0.6765\n",
      "Epoch 15/100, Loss: 0.6219, Acc: 0.7420, Val Loss: 0.7501, Val Acc: 0.6790\n",
      "Epoch 16/100, Loss: 0.6114, Acc: 0.7492, Val Loss: 0.7485, Val Acc: 0.6765\n",
      "Epoch 17/100, Loss: 0.6000, Acc: 0.7576, Val Loss: 0.7432, Val Acc: 0.6785\n",
      "Epoch 18/100, Loss: 0.5905, Acc: 0.7621, Val Loss: 0.7356, Val Acc: 0.6880\n",
      "Epoch 19/100, Loss: 0.5744, Acc: 0.7659, Val Loss: 0.7453, Val Acc: 0.6785\n",
      "Epoch 20/100, Loss: 0.5700, Acc: 0.7685, Val Loss: 0.7399, Val Acc: 0.6840\n",
      "Epoch 21/100, Loss: 0.5573, Acc: 0.7766, Val Loss: 0.7400, Val Acc: 0.6860\n",
      "Epoch 22/100, Loss: 0.5542, Acc: 0.7788, Val Loss: 0.7482, Val Acc: 0.6810\n",
      "Epoch 23/100, Loss: 0.5439, Acc: 0.7825, Val Loss: 0.7506, Val Acc: 0.6820\n",
      "Epoch 24/100, Loss: 0.5399, Acc: 0.7840, Val Loss: 0.7417, Val Acc: 0.6870\n",
      "Epoch 25/100, Loss: 0.5325, Acc: 0.7847, Val Loss: 0.7443, Val Acc: 0.6795\n",
      "Epoch 26/100, Loss: 0.5263, Acc: 0.7887, Val Loss: 0.7322, Val Acc: 0.6935\n",
      "Epoch 27/100, Loss: 0.5160, Acc: 0.7955, Val Loss: 0.7441, Val Acc: 0.6920\n",
      "Epoch 28/100, Loss: 0.5225, Acc: 0.7880, Val Loss: 0.7417, Val Acc: 0.6860\n",
      "Epoch 29/100, Loss: 0.5107, Acc: 0.7921, Val Loss: 0.7428, Val Acc: 0.6920\n",
      "Epoch 30/100, Loss: 0.5080, Acc: 0.8010, Val Loss: 0.7532, Val Acc: 0.6855\n",
      "Epoch 31/100, Loss: 0.5044, Acc: 0.7991, Val Loss: 0.7435, Val Acc: 0.6905\n",
      "Epoch 32/100, Loss: 0.5030, Acc: 0.8004, Val Loss: 0.7424, Val Acc: 0.6890\n",
      "Epoch 33/100, Loss: 0.4979, Acc: 0.7999, Val Loss: 0.7448, Val Acc: 0.6885\n",
      "Epoch 34/100, Loss: 0.4896, Acc: 0.8045, Val Loss: 0.7359, Val Acc: 0.7030\n",
      "Epoch 35/100, Loss: 0.4895, Acc: 0.8037, Val Loss: 0.7635, Val Acc: 0.6850\n",
      "Epoch 36/100, Loss: 0.4889, Acc: 0.8015, Val Loss: 0.7694, Val Acc: 0.6945\n",
      "Epoch 37/100, Loss: 0.4854, Acc: 0.8051, Val Loss: 0.7660, Val Acc: 0.6885\n",
      "Epoch 38/100, Loss: 0.4804, Acc: 0.8073, Val Loss: 0.7519, Val Acc: 0.6915\n",
      "Epoch 39/100, Loss: 0.4791, Acc: 0.8103, Val Loss: 0.7416, Val Acc: 0.7095\n",
      "Epoch 40/100, Loss: 0.4742, Acc: 0.8135, Val Loss: 0.7603, Val Acc: 0.6885\n",
      "Epoch 41/100, Loss: 0.4716, Acc: 0.8103, Val Loss: 0.7534, Val Acc: 0.6965\n",
      "Epoch 42/100, Loss: 0.4789, Acc: 0.8105, Val Loss: 0.7715, Val Acc: 0.6910\n",
      "Epoch 43/100, Loss: 0.4743, Acc: 0.8155, Val Loss: 0.7642, Val Acc: 0.6905\n",
      "Epoch 44/100, Loss: 0.4668, Acc: 0.8179, Val Loss: 0.7477, Val Acc: 0.7010\n",
      "Epoch 45/100, Loss: 0.4757, Acc: 0.8065, Val Loss: 0.7514, Val Acc: 0.7020\n",
      "Epoch 46/100, Loss: 0.4721, Acc: 0.8106, Val Loss: 0.7473, Val Acc: 0.7020\n",
      "Epoch 47/100, Loss: 0.4632, Acc: 0.8107, Val Loss: 0.7698, Val Acc: 0.6920\n",
      "Epoch 48/100, Loss: 0.4617, Acc: 0.8125, Val Loss: 0.7780, Val Acc: 0.6980\n",
      "Epoch 49/100, Loss: 0.4772, Acc: 0.8104, Val Loss: 0.7535, Val Acc: 0.7010\n",
      "Epoch 50/100, Loss: 0.4584, Acc: 0.8186, Val Loss: 0.7647, Val Acc: 0.7055\n",
      "Epoch 51/100, Loss: 0.4460, Acc: 0.8261, Val Loss: 0.7695, Val Acc: 0.7040\n",
      "Epoch 52/100, Loss: 0.4437, Acc: 0.8261, Val Loss: 0.7662, Val Acc: 0.7035\n",
      "Epoch 53/100, Loss: 0.4496, Acc: 0.8213, Val Loss: 0.7821, Val Acc: 0.7000\n",
      "Epoch 54/100, Loss: 0.4412, Acc: 0.8275, Val Loss: 0.7865, Val Acc: 0.7040\n",
      "Epoch 55/100, Loss: 0.4495, Acc: 0.8199, Val Loss: 0.7674, Val Acc: 0.7060\n",
      "Epoch 56/100, Loss: 0.4433, Acc: 0.8254, Val Loss: 0.7697, Val Acc: 0.7035\n",
      "Epoch 57/100, Loss: 0.4361, Acc: 0.8267, Val Loss: 0.7837, Val Acc: 0.6950\n",
      "Epoch 58/100, Loss: 0.4300, Acc: 0.8304, Val Loss: 0.7690, Val Acc: 0.7030\n",
      "Epoch 59/100, Loss: 0.4624, Acc: 0.8164, Val Loss: 0.8044, Val Acc: 0.6980\n",
      "Epoch 60/100, Loss: 0.4506, Acc: 0.8217, Val Loss: 0.7904, Val Acc: 0.7045\n",
      "Epoch 61/100, Loss: 0.4393, Acc: 0.8251, Val Loss: 0.7759, Val Acc: 0.7030\n",
      "Epoch 62/100, Loss: 0.4287, Acc: 0.8307, Val Loss: 0.7854, Val Acc: 0.6975\n",
      "Epoch 63/100, Loss: 0.4266, Acc: 0.8281, Val Loss: 0.7713, Val Acc: 0.7030\n",
      "Epoch 64/100, Loss: 0.4400, Acc: 0.8303, Val Loss: 0.7920, Val Acc: 0.7005\n",
      "Epoch 65/100, Loss: 0.4319, Acc: 0.8284, Val Loss: 0.7887, Val Acc: 0.7060\n",
      "Epoch 66/100, Loss: 0.4245, Acc: 0.8339, Val Loss: 0.7868, Val Acc: 0.7050\n",
      "Epoch 67/100, Loss: 0.4181, Acc: 0.8371, Val Loss: 0.7951, Val Acc: 0.6995\n",
      "Epoch 68/100, Loss: 0.4181, Acc: 0.8379, Val Loss: 0.7978, Val Acc: 0.7035\n",
      "Epoch 69/100, Loss: 0.4188, Acc: 0.8414, Val Loss: 0.7947, Val Acc: 0.7000\n",
      "Epoch 70/100, Loss: 0.4198, Acc: 0.8354, Val Loss: 0.8037, Val Acc: 0.6975\n",
      "Epoch 71/100, Loss: 0.4113, Acc: 0.8373, Val Loss: 0.7986, Val Acc: 0.6985\n",
      "Epoch 72/100, Loss: 0.4116, Acc: 0.8379, Val Loss: 0.7939, Val Acc: 0.6980\n",
      "Epoch 73/100, Loss: 0.4079, Acc: 0.8393, Val Loss: 0.8073, Val Acc: 0.6955\n",
      "Epoch 74/100, Loss: 0.4097, Acc: 0.8391, Val Loss: 0.8024, Val Acc: 0.6950\n",
      "Epoch 75/100, Loss: 0.4074, Acc: 0.8423, Val Loss: 0.7925, Val Acc: 0.7085\n",
      "Epoch 76/100, Loss: 0.4080, Acc: 0.8421, Val Loss: 0.7916, Val Acc: 0.7115\n",
      "Epoch 77/100, Loss: 0.4100, Acc: 0.8397, Val Loss: 0.7941, Val Acc: 0.7040\n",
      "Epoch 78/100, Loss: 0.4068, Acc: 0.8381, Val Loss: 0.8000, Val Acc: 0.7060\n",
      "Epoch 79/100, Loss: 0.4268, Acc: 0.8299, Val Loss: 0.7991, Val Acc: 0.7030\n",
      "Epoch 80/100, Loss: 0.4177, Acc: 0.8361, Val Loss: 0.7908, Val Acc: 0.7035\n",
      "Epoch 81/100, Loss: 0.4034, Acc: 0.8427, Val Loss: 0.8124, Val Acc: 0.7000\n",
      "Epoch 82/100, Loss: 0.3933, Acc: 0.8509, Val Loss: 0.8062, Val Acc: 0.6985\n",
      "Epoch 83/100, Loss: 0.3936, Acc: 0.8484, Val Loss: 0.8006, Val Acc: 0.7020\n",
      "Epoch 84/100, Loss: 0.3929, Acc: 0.8445, Val Loss: 0.8037, Val Acc: 0.7015\n",
      "Epoch 85/100, Loss: 0.3872, Acc: 0.8512, Val Loss: 0.8189, Val Acc: 0.6945\n",
      "Epoch 86/100, Loss: 0.3941, Acc: 0.8452, Val Loss: 0.8154, Val Acc: 0.6970\n",
      "Epoch 87/100, Loss: 0.3946, Acc: 0.8458, Val Loss: 0.8302, Val Acc: 0.6945\n",
      "Epoch 88/100, Loss: 0.3881, Acc: 0.8471, Val Loss: 0.8460, Val Acc: 0.6980\n",
      "Epoch 89/100, Loss: 0.3877, Acc: 0.8452, Val Loss: 0.8415, Val Acc: 0.6955\n",
      "Epoch 90/100, Loss: 0.3999, Acc: 0.8414, Val Loss: 0.8362, Val Acc: 0.6965\n",
      "Epoch 91/100, Loss: 0.3894, Acc: 0.8478, Val Loss: 0.8332, Val Acc: 0.6985\n",
      "Epoch 92/100, Loss: 0.3874, Acc: 0.8504, Val Loss: 0.8427, Val Acc: 0.6995\n",
      "Epoch 93/100, Loss: 0.3957, Acc: 0.8470, Val Loss: 0.8467, Val Acc: 0.7005\n",
      "Epoch 94/100, Loss: 0.3922, Acc: 0.8451, Val Loss: 0.8405, Val Acc: 0.7005\n",
      "Epoch 95/100, Loss: 0.4041, Acc: 0.8429, Val Loss: 0.8363, Val Acc: 0.7045\n",
      "Epoch 96/100, Loss: 0.3922, Acc: 0.8464, Val Loss: 0.8212, Val Acc: 0.7015\n",
      "Epoch 97/100, Loss: 0.3822, Acc: 0.8525, Val Loss: 0.8238, Val Acc: 0.6980\n",
      "Epoch 98/100, Loss: 0.3715, Acc: 0.8575, Val Loss: 0.8287, Val Acc: 0.7085\n",
      "Epoch 99/100, Loss: 0.3766, Acc: 0.8542, Val Loss: 0.8449, Val Acc: 0.6985\n",
      "Epoch 100/100, Loss: 0.3841, Acc: 0.8491, Val Loss: 0.8457, Val Acc: 0.6920\n",
      "✅ Accuracy : 0.6920\n",
      "✅ Precision: 0.4634\n",
      "✅ Recall   : 0.4900\n",
      "✅ F1 Score : 0.4744\n",
      "\n",
      "🔁 Running config 10/12\n",
      "Layers: [179, 16, 8, 7], Activations: ['relu', 'relu', 'softmax'], LR: 0.1\n",
      "Epoch 1/100, Loss: 1.6483, Acc: 0.2923, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 2/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 3/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 4/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 5/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 6/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 7/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 8/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 9/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 10/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 11/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 12/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 13/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 14/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 15/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 16/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 17/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 18/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 19/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 20/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 21/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 22/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 23/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 24/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 25/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 26/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 27/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 28/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 29/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 30/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 31/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 32/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 33/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 34/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 35/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 36/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 37/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 38/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 39/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 40/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 41/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 42/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 43/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 44/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 45/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 46/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 47/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 48/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 49/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 50/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 51/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 52/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 53/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 54/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 55/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 56/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 57/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 58/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 59/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 60/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 61/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 62/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 63/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 64/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 65/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 66/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 67/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 68/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 69/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 70/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 71/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 72/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 73/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 74/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 75/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 76/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 77/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 78/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 79/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 80/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 81/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 82/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 83/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 84/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 85/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 86/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 87/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 88/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 89/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 90/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 91/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 92/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 93/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 94/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 95/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 96/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 97/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 98/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 99/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "Epoch 100/100, Loss: 1.6020, Acc: 0.2920, Val Loss: 1.6082, Val Acc: 0.2935\n",
      "✅ Accuracy : 0.2935\n",
      "✅ Precision: 0.0419\n",
      "✅ Recall   : 0.1429\n",
      "✅ F1 Score : 0.0648\n",
      "\n",
      "🔁 Running config 11/12\n",
      "Layers: [179, 16, 8, 7], Activations: ['tanh', 'relu', 'softmax'], LR: 0.1\n",
      "Epoch 1/100, Loss: 1.3710, Acc: 0.4295, Val Loss: 1.1630, Val Acc: 0.5245\n",
      "Epoch 2/100, Loss: 1.1717, Acc: 0.5165, Val Loss: 1.0512, Val Acc: 0.5580\n",
      "Epoch 3/100, Loss: 1.1555, Acc: 0.5268, Val Loss: 1.0814, Val Acc: 0.5500\n",
      "Epoch 4/100, Loss: 1.1506, Acc: 0.5282, Val Loss: 1.0914, Val Acc: 0.5795\n",
      "Epoch 5/100, Loss: 1.1515, Acc: 0.5209, Val Loss: 1.1322, Val Acc: 0.5330\n",
      "Epoch 6/100, Loss: 1.1335, Acc: 0.5370, Val Loss: 0.9696, Val Acc: 0.6060\n",
      "Epoch 7/100, Loss: 1.1338, Acc: 0.5549, Val Loss: 1.1042, Val Acc: 0.5395\n",
      "Epoch 8/100, Loss: 1.0889, Acc: 0.5833, Val Loss: 1.0473, Val Acc: 0.6040\n",
      "Epoch 9/100, Loss: 1.0841, Acc: 0.5799, Val Loss: 0.9404, Val Acc: 0.6130\n",
      "Epoch 10/100, Loss: 1.1166, Acc: 0.5607, Val Loss: 0.9330, Val Acc: 0.6300\n",
      "Epoch 11/100, Loss: 1.0850, Acc: 0.5740, Val Loss: 0.9977, Val Acc: 0.5940\n",
      "Epoch 12/100, Loss: 1.0918, Acc: 0.5880, Val Loss: 1.0436, Val Acc: 0.5740\n",
      "Epoch 13/100, Loss: 1.0788, Acc: 0.5736, Val Loss: 1.1116, Val Acc: 0.5810\n",
      "Epoch 14/100, Loss: 1.1043, Acc: 0.5836, Val Loss: 0.9922, Val Acc: 0.5810\n",
      "Epoch 15/100, Loss: 1.1996, Acc: 0.5437, Val Loss: 1.3014, Val Acc: 0.4100\n",
      "Epoch 16/100, Loss: 1.1490, Acc: 0.5637, Val Loss: 0.9968, Val Acc: 0.6085\n",
      "Epoch 17/100, Loss: 1.1286, Acc: 0.5640, Val Loss: 1.1527, Val Acc: 0.5405\n",
      "Epoch 18/100, Loss: 1.2343, Acc: 0.5229, Val Loss: 1.2170, Val Acc: 0.5075\n",
      "Epoch 19/100, Loss: 1.2404, Acc: 0.5200, Val Loss: 1.1847, Val Acc: 0.5400\n",
      "Epoch 20/100, Loss: 1.2359, Acc: 0.5201, Val Loss: 1.2755, Val Acc: 0.5230\n",
      "Epoch 21/100, Loss: 1.2563, Acc: 0.5050, Val Loss: 1.2915, Val Acc: 0.4685\n",
      "Epoch 22/100, Loss: 1.2509, Acc: 0.5209, Val Loss: 1.2498, Val Acc: 0.5030\n",
      "Epoch 23/100, Loss: 1.2323, Acc: 0.5288, Val Loss: 1.2072, Val Acc: 0.4150\n",
      "Epoch 24/100, Loss: 1.2340, Acc: 0.5294, Val Loss: 1.1992, Val Acc: 0.5280\n",
      "Epoch 25/100, Loss: 1.2403, Acc: 0.5110, Val Loss: 1.2745, Val Acc: 0.4240\n",
      "Epoch 26/100, Loss: 1.2511, Acc: 0.5230, Val Loss: 1.4439, Val Acc: 0.4535\n",
      "Epoch 27/100, Loss: 1.2320, Acc: 0.5226, Val Loss: 1.1832, Val Acc: 0.5240\n",
      "Epoch 28/100, Loss: 1.2280, Acc: 0.5186, Val Loss: 1.3651, Val Acc: 0.5235\n",
      "Epoch 29/100, Loss: 1.2422, Acc: 0.5270, Val Loss: 1.3353, Val Acc: 0.5085\n",
      "Epoch 30/100, Loss: 1.1985, Acc: 0.5433, Val Loss: 1.2090, Val Acc: 0.5610\n",
      "Epoch 31/100, Loss: 1.2099, Acc: 0.5393, Val Loss: 1.2712, Val Acc: 0.5005\n",
      "Epoch 32/100, Loss: 1.1996, Acc: 0.5513, Val Loss: 1.1122, Val Acc: 0.5840\n",
      "Epoch 33/100, Loss: 1.2019, Acc: 0.5421, Val Loss: 1.2255, Val Acc: 0.5255\n",
      "Epoch 34/100, Loss: 1.2161, Acc: 0.5413, Val Loss: 1.1110, Val Acc: 0.5545\n",
      "Epoch 35/100, Loss: 1.2306, Acc: 0.5354, Val Loss: 1.1485, Val Acc: 0.5485\n",
      "Epoch 36/100, Loss: 1.2368, Acc: 0.5309, Val Loss: 1.3252, Val Acc: 0.4540\n",
      "Epoch 37/100, Loss: 1.2199, Acc: 0.5454, Val Loss: 1.2097, Val Acc: 0.4615\n",
      "Epoch 38/100, Loss: 1.1859, Acc: 0.5459, Val Loss: 1.3690, Val Acc: 0.4175\n",
      "Epoch 39/100, Loss: 1.2195, Acc: 0.5314, Val Loss: 1.3336, Val Acc: 0.3740\n",
      "Epoch 40/100, Loss: 1.2112, Acc: 0.5395, Val Loss: 1.1874, Val Acc: 0.5380\n",
      "Epoch 41/100, Loss: 1.2243, Acc: 0.5385, Val Loss: 1.4224, Val Acc: 0.4820\n",
      "Epoch 42/100, Loss: 1.1971, Acc: 0.5316, Val Loss: 1.4142, Val Acc: 0.4670\n",
      "Epoch 43/100, Loss: 1.1943, Acc: 0.5375, Val Loss: 1.2465, Val Acc: 0.4460\n",
      "Epoch 44/100, Loss: 1.1758, Acc: 0.5405, Val Loss: 1.3085, Val Acc: 0.5040\n",
      "Epoch 45/100, Loss: 1.1885, Acc: 0.5196, Val Loss: 1.1409, Val Acc: 0.4625\n",
      "Epoch 46/100, Loss: 1.1907, Acc: 0.5363, Val Loss: 1.1014, Val Acc: 0.5505\n",
      "Epoch 47/100, Loss: 1.2137, Acc: 0.5311, Val Loss: 1.2174, Val Acc: 0.5265\n",
      "Epoch 48/100, Loss: 1.1822, Acc: 0.5365, Val Loss: 1.1302, Val Acc: 0.5015\n",
      "Epoch 49/100, Loss: 1.2051, Acc: 0.5327, Val Loss: 1.1596, Val Acc: 0.4175\n",
      "Epoch 50/100, Loss: 1.1721, Acc: 0.5394, Val Loss: 1.0918, Val Acc: 0.4690\n",
      "Epoch 51/100, Loss: 1.1652, Acc: 0.5437, Val Loss: 1.3004, Val Acc: 0.4025\n",
      "Epoch 52/100, Loss: 1.1781, Acc: 0.5404, Val Loss: 1.2365, Val Acc: 0.4500\n",
      "Epoch 53/100, Loss: 1.1978, Acc: 0.5430, Val Loss: 1.2872, Val Acc: 0.5650\n",
      "Epoch 54/100, Loss: 1.1690, Acc: 0.5443, Val Loss: 1.1936, Val Acc: 0.4355\n",
      "Epoch 55/100, Loss: 1.2275, Acc: 0.5390, Val Loss: 1.4428, Val Acc: 0.5230\n",
      "Epoch 56/100, Loss: 1.2283, Acc: 0.5336, Val Loss: 1.4361, Val Acc: 0.3820\n",
      "Epoch 57/100, Loss: 1.2118, Acc: 0.5324, Val Loss: 1.1668, Val Acc: 0.5105\n",
      "Epoch 58/100, Loss: 1.1592, Acc: 0.5447, Val Loss: 1.1554, Val Acc: 0.5725\n",
      "Epoch 59/100, Loss: 1.1942, Acc: 0.5381, Val Loss: 1.1442, Val Acc: 0.5800\n",
      "Epoch 60/100, Loss: 1.1858, Acc: 0.5599, Val Loss: 1.1642, Val Acc: 0.5740\n",
      "Epoch 61/100, Loss: 1.2299, Acc: 0.5453, Val Loss: 1.3005, Val Acc: 0.5000\n",
      "Epoch 62/100, Loss: 1.1992, Acc: 0.5536, Val Loss: 1.3112, Val Acc: 0.5380\n",
      "Epoch 63/100, Loss: 1.1806, Acc: 0.5489, Val Loss: 1.3824, Val Acc: 0.4865\n",
      "Epoch 64/100, Loss: 1.1732, Acc: 0.5554, Val Loss: 1.2772, Val Acc: 0.5745\n",
      "Epoch 65/100, Loss: 1.1852, Acc: 0.5573, Val Loss: 1.3806, Val Acc: 0.5420\n",
      "Epoch 66/100, Loss: 1.1865, Acc: 0.5577, Val Loss: 1.1959, Val Acc: 0.4360\n",
      "Epoch 67/100, Loss: 1.1734, Acc: 0.5496, Val Loss: 1.1987, Val Acc: 0.5440\n",
      "Epoch 68/100, Loss: 1.1845, Acc: 0.5514, Val Loss: 1.3044, Val Acc: 0.4465\n",
      "Epoch 69/100, Loss: 1.2270, Acc: 0.5315, Val Loss: 1.2323, Val Acc: 0.4170\n",
      "Epoch 70/100, Loss: 1.2211, Acc: 0.5336, Val Loss: 1.1642, Val Acc: 0.4640\n",
      "Epoch 71/100, Loss: 1.1866, Acc: 0.5404, Val Loss: 1.2093, Val Acc: 0.4530\n",
      "Epoch 72/100, Loss: 1.2055, Acc: 0.5374, Val Loss: 1.1465, Val Acc: 0.4985\n",
      "Epoch 73/100, Loss: 1.2001, Acc: 0.5337, Val Loss: 1.2392, Val Acc: 0.5230\n",
      "Epoch 74/100, Loss: 1.1897, Acc: 0.5447, Val Loss: 1.1478, Val Acc: 0.5125\n",
      "Epoch 75/100, Loss: 1.1630, Acc: 0.5453, Val Loss: 1.2151, Val Acc: 0.5770\n",
      "Epoch 76/100, Loss: 1.1970, Acc: 0.5353, Val Loss: 1.1849, Val Acc: 0.4775\n",
      "Epoch 77/100, Loss: 1.1802, Acc: 0.5397, Val Loss: 1.1507, Val Acc: 0.5580\n",
      "Epoch 78/100, Loss: 1.1898, Acc: 0.5316, Val Loss: 1.1995, Val Acc: 0.5095\n",
      "Epoch 79/100, Loss: 1.1896, Acc: 0.5426, Val Loss: 1.2413, Val Acc: 0.4590\n",
      "Epoch 80/100, Loss: 1.1878, Acc: 0.5431, Val Loss: 1.1774, Val Acc: 0.5440\n",
      "Epoch 81/100, Loss: 1.1850, Acc: 0.5199, Val Loss: 1.1704, Val Acc: 0.5500\n",
      "Epoch 82/100, Loss: 1.2078, Acc: 0.5238, Val Loss: 1.4582, Val Acc: 0.4345\n",
      "Epoch 83/100, Loss: 1.1965, Acc: 0.5427, Val Loss: 1.4096, Val Acc: 0.4910\n",
      "Epoch 84/100, Loss: 1.2115, Acc: 0.5383, Val Loss: 1.2327, Val Acc: 0.4735\n",
      "Epoch 85/100, Loss: 1.1834, Acc: 0.5467, Val Loss: 1.0333, Val Acc: 0.5615\n",
      "Epoch 86/100, Loss: 1.1712, Acc: 0.5529, Val Loss: 1.1934, Val Acc: 0.5350\n",
      "Epoch 87/100, Loss: 1.1776, Acc: 0.5504, Val Loss: 1.2723, Val Acc: 0.4470\n",
      "Epoch 88/100, Loss: 1.1916, Acc: 0.5435, Val Loss: 1.6142, Val Acc: 0.4705\n",
      "Epoch 89/100, Loss: 1.1466, Acc: 0.5551, Val Loss: 0.9934, Val Acc: 0.5820\n",
      "Epoch 90/100, Loss: 1.1609, Acc: 0.5416, Val Loss: 1.0986, Val Acc: 0.6010\n",
      "Epoch 91/100, Loss: 1.1628, Acc: 0.5441, Val Loss: 1.1298, Val Acc: 0.5505\n",
      "Epoch 92/100, Loss: 1.1323, Acc: 0.5543, Val Loss: 1.1370, Val Acc: 0.5405\n",
      "Epoch 93/100, Loss: 1.1488, Acc: 0.5576, Val Loss: 1.2788, Val Acc: 0.4960\n",
      "Epoch 94/100, Loss: 1.1490, Acc: 0.5605, Val Loss: 1.1523, Val Acc: 0.5530\n",
      "Epoch 95/100, Loss: 1.1616, Acc: 0.5566, Val Loss: 1.5661, Val Acc: 0.5245\n",
      "Epoch 96/100, Loss: 1.1819, Acc: 0.5554, Val Loss: 1.3644, Val Acc: 0.5260\n",
      "Epoch 97/100, Loss: 1.1697, Acc: 0.5475, Val Loss: 1.1649, Val Acc: 0.5335\n",
      "Epoch 98/100, Loss: 1.1563, Acc: 0.5517, Val Loss: 1.1818, Val Acc: 0.4815\n",
      "Epoch 99/100, Loss: 1.1669, Acc: 0.5435, Val Loss: 1.1548, Val Acc: 0.5550\n",
      "Epoch 100/100, Loss: 1.2126, Acc: 0.5424, Val Loss: 1.5863, Val Acc: 0.5065\n",
      "✅ Accuracy : 0.5065\n",
      "✅ Precision: 0.2830\n",
      "✅ Recall   : 0.2618\n",
      "✅ F1 Score : 0.2295\n",
      "\n",
      "🔁 Running config 12/12\n",
      "Layers: [179, 16, 8, 7], Activations: ['tanh', 'tanh', 'softmax'], LR: 0.1\n",
      "Epoch 1/100, Loss: 1.2846, Acc: 0.4334, Val Loss: 1.1089, Val Acc: 0.4945\n",
      "Epoch 2/100, Loss: 1.0236, Acc: 0.5405, Val Loss: 0.9410, Val Acc: 0.5860\n",
      "Epoch 3/100, Loss: 0.9648, Acc: 0.5797, Val Loss: 0.9231, Val Acc: 0.5860\n",
      "Epoch 4/100, Loss: 0.9145, Acc: 0.6075, Val Loss: 0.9516, Val Acc: 0.5705\n",
      "Epoch 5/100, Loss: 0.8921, Acc: 0.6259, Val Loss: 0.8688, Val Acc: 0.6260\n",
      "Epoch 6/100, Loss: 0.8726, Acc: 0.6282, Val Loss: 0.8619, Val Acc: 0.6340\n",
      "Epoch 7/100, Loss: 0.8786, Acc: 0.6182, Val Loss: 0.9283, Val Acc: 0.5770\n",
      "Epoch 8/100, Loss: 0.8535, Acc: 0.6340, Val Loss: 0.9727, Val Acc: 0.6235\n",
      "Epoch 9/100, Loss: 0.8631, Acc: 0.6376, Val Loss: 0.8491, Val Acc: 0.6610\n",
      "Epoch 10/100, Loss: 0.8272, Acc: 0.6550, Val Loss: 0.8672, Val Acc: 0.6435\n",
      "Epoch 11/100, Loss: 0.8250, Acc: 0.6459, Val Loss: 0.8723, Val Acc: 0.6130\n",
      "Epoch 12/100, Loss: 0.8103, Acc: 0.6584, Val Loss: 0.8836, Val Acc: 0.6845\n",
      "Epoch 13/100, Loss: 0.8205, Acc: 0.6609, Val Loss: 0.8367, Val Acc: 0.6700\n",
      "Epoch 14/100, Loss: 0.7952, Acc: 0.6706, Val Loss: 0.8626, Val Acc: 0.6550\n",
      "Epoch 15/100, Loss: 0.7984, Acc: 0.6625, Val Loss: 0.8000, Val Acc: 0.6825\n",
      "Epoch 16/100, Loss: 0.7817, Acc: 0.6751, Val Loss: 0.8155, Val Acc: 0.6670\n",
      "Epoch 17/100, Loss: 0.7679, Acc: 0.6857, Val Loss: 0.8479, Val Acc: 0.6685\n",
      "Epoch 18/100, Loss: 0.7787, Acc: 0.6687, Val Loss: 0.7802, Val Acc: 0.6790\n",
      "Epoch 19/100, Loss: 0.7735, Acc: 0.6727, Val Loss: 0.7736, Val Acc: 0.6895\n",
      "Epoch 20/100, Loss: 0.7804, Acc: 0.6756, Val Loss: 0.8268, Val Acc: 0.6715\n",
      "Epoch 21/100, Loss: 0.7769, Acc: 0.6786, Val Loss: 0.8019, Val Acc: 0.6905\n",
      "Epoch 22/100, Loss: 0.7745, Acc: 0.6805, Val Loss: 0.8754, Val Acc: 0.6320\n",
      "Epoch 23/100, Loss: 0.7700, Acc: 0.6850, Val Loss: 0.8227, Val Acc: 0.6595\n",
      "Epoch 24/100, Loss: 0.7723, Acc: 0.6801, Val Loss: 0.7875, Val Acc: 0.6630\n",
      "Epoch 25/100, Loss: 0.7623, Acc: 0.6825, Val Loss: 0.7622, Val Acc: 0.6995\n",
      "Epoch 26/100, Loss: 0.7692, Acc: 0.6765, Val Loss: 0.7934, Val Acc: 0.6885\n",
      "Epoch 27/100, Loss: 0.7448, Acc: 0.6765, Val Loss: 0.7910, Val Acc: 0.6775\n",
      "Epoch 28/100, Loss: 0.7474, Acc: 0.6901, Val Loss: 0.7821, Val Acc: 0.6600\n",
      "Epoch 29/100, Loss: 0.7559, Acc: 0.6821, Val Loss: 0.7785, Val Acc: 0.6765\n",
      "Epoch 30/100, Loss: 0.7509, Acc: 0.6845, Val Loss: 0.8330, Val Acc: 0.6680\n",
      "Epoch 31/100, Loss: 0.7495, Acc: 0.6841, Val Loss: 0.8376, Val Acc: 0.6635\n",
      "Epoch 32/100, Loss: 0.7418, Acc: 0.6864, Val Loss: 0.8065, Val Acc: 0.6775\n",
      "Epoch 33/100, Loss: 0.7477, Acc: 0.6869, Val Loss: 0.7173, Val Acc: 0.7055\n",
      "Epoch 34/100, Loss: 0.7338, Acc: 0.6955, Val Loss: 0.7561, Val Acc: 0.7005\n",
      "Epoch 35/100, Loss: 0.7546, Acc: 0.6839, Val Loss: 0.7415, Val Acc: 0.7070\n",
      "Epoch 36/100, Loss: 0.7088, Acc: 0.7095, Val Loss: 0.8412, Val Acc: 0.6790\n",
      "Epoch 37/100, Loss: 0.7272, Acc: 0.7007, Val Loss: 0.7855, Val Acc: 0.7005\n",
      "Epoch 38/100, Loss: 0.7127, Acc: 0.7006, Val Loss: 0.7998, Val Acc: 0.6915\n",
      "Epoch 39/100, Loss: 0.7221, Acc: 0.6954, Val Loss: 0.7706, Val Acc: 0.6875\n",
      "Epoch 40/100, Loss: 0.7274, Acc: 0.7037, Val Loss: 0.8112, Val Acc: 0.6870\n",
      "Epoch 41/100, Loss: 0.7313, Acc: 0.7000, Val Loss: 0.8091, Val Acc: 0.7000\n",
      "Epoch 42/100, Loss: 0.7156, Acc: 0.6927, Val Loss: 0.8628, Val Acc: 0.6525\n",
      "Epoch 43/100, Loss: 0.7109, Acc: 0.7120, Val Loss: 0.8097, Val Acc: 0.6740\n",
      "Epoch 44/100, Loss: 0.7127, Acc: 0.6984, Val Loss: 0.8281, Val Acc: 0.6960\n",
      "Epoch 45/100, Loss: 0.7050, Acc: 0.7049, Val Loss: 0.7091, Val Acc: 0.7095\n",
      "Epoch 46/100, Loss: 0.7093, Acc: 0.7090, Val Loss: 0.7406, Val Acc: 0.7285\n",
      "Epoch 47/100, Loss: 0.6972, Acc: 0.7150, Val Loss: 0.7263, Val Acc: 0.7200\n",
      "Epoch 48/100, Loss: 0.7070, Acc: 0.7077, Val Loss: 0.7432, Val Acc: 0.7175\n",
      "Epoch 49/100, Loss: 0.6831, Acc: 0.7221, Val Loss: 0.6985, Val Acc: 0.6995\n",
      "Epoch 50/100, Loss: 0.7027, Acc: 0.7173, Val Loss: 0.7115, Val Acc: 0.7290\n",
      "Epoch 51/100, Loss: 0.7067, Acc: 0.7090, Val Loss: 0.8127, Val Acc: 0.6890\n",
      "Epoch 52/100, Loss: 0.7121, Acc: 0.7050, Val Loss: 0.7409, Val Acc: 0.7030\n",
      "Epoch 53/100, Loss: 0.6856, Acc: 0.7238, Val Loss: 0.7887, Val Acc: 0.7045\n",
      "Epoch 54/100, Loss: 0.7119, Acc: 0.7180, Val Loss: 0.9072, Val Acc: 0.6645\n",
      "Epoch 55/100, Loss: 0.6988, Acc: 0.7215, Val Loss: 0.8139, Val Acc: 0.6440\n",
      "Epoch 56/100, Loss: 0.6933, Acc: 0.7179, Val Loss: 0.7421, Val Acc: 0.7215\n",
      "Epoch 57/100, Loss: 0.6824, Acc: 0.7186, Val Loss: 0.7277, Val Acc: 0.7080\n",
      "Epoch 58/100, Loss: 0.6893, Acc: 0.7141, Val Loss: 0.7118, Val Acc: 0.7305\n",
      "Epoch 59/100, Loss: 0.6749, Acc: 0.7204, Val Loss: 0.7192, Val Acc: 0.7245\n",
      "Epoch 60/100, Loss: 0.6674, Acc: 0.7339, Val Loss: 0.7379, Val Acc: 0.7140\n",
      "Epoch 61/100, Loss: 0.6692, Acc: 0.7271, Val Loss: 0.7607, Val Acc: 0.7020\n",
      "Epoch 62/100, Loss: 0.6606, Acc: 0.7339, Val Loss: 0.6890, Val Acc: 0.7275\n",
      "Epoch 63/100, Loss: 0.6661, Acc: 0.7276, Val Loss: 0.7243, Val Acc: 0.7275\n",
      "Epoch 64/100, Loss: 0.6711, Acc: 0.7270, Val Loss: 0.7009, Val Acc: 0.7235\n",
      "Epoch 65/100, Loss: 0.6767, Acc: 0.7258, Val Loss: 0.7622, Val Acc: 0.7155\n",
      "Epoch 66/100, Loss: 0.6694, Acc: 0.7245, Val Loss: 0.7238, Val Acc: 0.6910\n",
      "Epoch 67/100, Loss: 0.6732, Acc: 0.7270, Val Loss: 0.7313, Val Acc: 0.7160\n",
      "Epoch 68/100, Loss: 0.6742, Acc: 0.7195, Val Loss: 0.6954, Val Acc: 0.7345\n",
      "Epoch 69/100, Loss: 0.6732, Acc: 0.7224, Val Loss: 0.7118, Val Acc: 0.7250\n",
      "Epoch 70/100, Loss: 0.6741, Acc: 0.7196, Val Loss: 0.6917, Val Acc: 0.7435\n",
      "Epoch 71/100, Loss: 0.6742, Acc: 0.7359, Val Loss: 0.6658, Val Acc: 0.7320\n",
      "Epoch 72/100, Loss: 0.6645, Acc: 0.7324, Val Loss: 0.7471, Val Acc: 0.7050\n",
      "Epoch 73/100, Loss: 0.6572, Acc: 0.7412, Val Loss: 0.7208, Val Acc: 0.7255\n",
      "Epoch 74/100, Loss: 0.6656, Acc: 0.7338, Val Loss: 0.7245, Val Acc: 0.7325\n",
      "Epoch 75/100, Loss: 0.6746, Acc: 0.7250, Val Loss: 0.7246, Val Acc: 0.7345\n",
      "Epoch 76/100, Loss: 0.6674, Acc: 0.7299, Val Loss: 0.7309, Val Acc: 0.7320\n",
      "Epoch 77/100, Loss: 0.6704, Acc: 0.7301, Val Loss: 0.6818, Val Acc: 0.7390\n",
      "Epoch 78/100, Loss: 0.6840, Acc: 0.7236, Val Loss: 0.6736, Val Acc: 0.7185\n",
      "Epoch 79/100, Loss: 0.6894, Acc: 0.7201, Val Loss: 0.6999, Val Acc: 0.7105\n",
      "Epoch 80/100, Loss: 0.6722, Acc: 0.7345, Val Loss: 0.7287, Val Acc: 0.6940\n",
      "Epoch 81/100, Loss: 0.6636, Acc: 0.7329, Val Loss: 0.7138, Val Acc: 0.7210\n",
      "Epoch 82/100, Loss: 0.6831, Acc: 0.7285, Val Loss: 0.7441, Val Acc: 0.7250\n",
      "Epoch 83/100, Loss: 0.7092, Acc: 0.7093, Val Loss: 0.7163, Val Acc: 0.7175\n",
      "Epoch 84/100, Loss: 0.7089, Acc: 0.7080, Val Loss: 0.7575, Val Acc: 0.7050\n",
      "Epoch 85/100, Loss: 0.6875, Acc: 0.7298, Val Loss: 0.7189, Val Acc: 0.7125\n",
      "Epoch 86/100, Loss: 0.6835, Acc: 0.7280, Val Loss: 0.7192, Val Acc: 0.7155\n",
      "Epoch 87/100, Loss: 0.6732, Acc: 0.7225, Val Loss: 0.8545, Val Acc: 0.6675\n",
      "Epoch 88/100, Loss: 0.6748, Acc: 0.7198, Val Loss: 0.7327, Val Acc: 0.7210\n",
      "Epoch 89/100, Loss: 0.6846, Acc: 0.7104, Val Loss: 0.7488, Val Acc: 0.7220\n",
      "Epoch 90/100, Loss: 0.6828, Acc: 0.7226, Val Loss: 0.7863, Val Acc: 0.7025\n",
      "Epoch 91/100, Loss: 0.6739, Acc: 0.7265, Val Loss: 0.6833, Val Acc: 0.7375\n",
      "Epoch 92/100, Loss: 0.6657, Acc: 0.7326, Val Loss: 0.7559, Val Acc: 0.7040\n",
      "Epoch 93/100, Loss: 0.6708, Acc: 0.7260, Val Loss: 0.7634, Val Acc: 0.6530\n",
      "Epoch 94/100, Loss: 0.6696, Acc: 0.7321, Val Loss: 0.7219, Val Acc: 0.7225\n",
      "Epoch 95/100, Loss: 0.6683, Acc: 0.7238, Val Loss: 0.7263, Val Acc: 0.7155\n",
      "Epoch 96/100, Loss: 0.6535, Acc: 0.7336, Val Loss: 0.7388, Val Acc: 0.7025\n",
      "Epoch 97/100, Loss: 0.6645, Acc: 0.7285, Val Loss: 0.7027, Val Acc: 0.6960\n",
      "Epoch 98/100, Loss: 0.6550, Acc: 0.7358, Val Loss: 0.7568, Val Acc: 0.7005\n",
      "Epoch 99/100, Loss: 0.6610, Acc: 0.7352, Val Loss: 0.7528, Val Acc: 0.6920\n",
      "Epoch 100/100, Loss: 0.6674, Acc: 0.7298, Val Loss: 0.7090, Val Acc: 0.7215\n",
      "✅ Accuracy : 0.7215\n",
      "✅ Precision: 0.4929\n",
      "✅ Recall   : 0.5154\n",
      "✅ F1 Score : 0.5016\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, cfg in enumerate(configs):\n",
    "    print(f\"\\n🔁 Running config {i+1}/{len(configs)}\")\n",
    "    print(f\"Layers: {cfg['layers']}, Activations: {[fn.__name__ for fn in cfg['act']]}, LR: {cfg['lr']}\")\n",
    "\n",
    "    nn = NeuralNetwork(cfg[\"layers\"], cfg[\"act\"], learning_rate=cfg[\"lr\"],multiclass=True)\n",
    "    \n",
    "    # Train including validation set\n",
    "    nn.train(X_train, y_train, epochs=100, X_val=X_val, y_val=y_val)\n",
    "    \n",
    "    # Predict final output\n",
    "    preds = nn.predict(X_val)  # output: class labels\n",
    "    y_val_labels = np.argmax(y_val, axis=1)  # one-hot → class index\n",
    "\n",
    "    metrics = evaluate_classification_metrics(y_val_labels, preds, multiclass=True)\n",
    "\n",
    "    \n",
    "    # Get metrics stored during training from the model\n",
    "    train_loss = nn.history[\"train_loss\"][-1]\n",
    "    val_loss = nn.history[\"val_loss\"][-1] if nn.history[\"val_loss\"] else None\n",
    "    train_acc = nn.history[\"train_acc\"][-1]\n",
    "    val_acc = nn.history[\"val_acc\"][-1] if nn.history[\"val_acc\"] else None\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"config_id\": i + 1,\n",
    "        \"layers\": cfg[\"layers\"],\n",
    "        \"activations\": [fn.__name__ for fn in cfg[\"act\"]],\n",
    "        \"learning_rate\": cfg[\"lr\"],\n",
    "\n",
    "        # Add train history\n",
    "        \"train_loss\": float(train_loss),\n",
    "        \"val_loss\": float(val_loss) if val_loss is not None else None,\n",
    "        \"train_acc\": float(train_acc),\n",
    "        \"val_acc\": float(val_acc) if val_acc is not None else None,\n",
    "\n",
    "        # Add evaluation metrics from evaluate_classification_metrics()\n",
    "        **metrics})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d33fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_id</th>\n",
       "      <th>layers</th>\n",
       "      <th>activations</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[179, 7]</td>\n",
       "      <td>[softmax]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.270317</td>\n",
       "      <td>0.427863</td>\n",
       "      <td>0.892375</td>\n",
       "      <td>0.8405</td>\n",
       "      <td>0.8405</td>\n",
       "      <td>0.714092</td>\n",
       "      <td>0.706593</td>\n",
       "      <td>0.707552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[179, 7]</td>\n",
       "      <td>[softmax]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.057552</td>\n",
       "      <td>1.660113</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.700871</td>\n",
       "      <td>0.710071</td>\n",
       "      <td>0.700474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[179, 16, 7]</td>\n",
       "      <td>[relu, softmax]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.269263</td>\n",
       "      <td>0.601914</td>\n",
       "      <td>0.903375</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.716525</td>\n",
       "      <td>0.746271</td>\n",
       "      <td>0.701078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[179, 16, 7]</td>\n",
       "      <td>[tanh, softmax]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.227352</td>\n",
       "      <td>0.700322</td>\n",
       "      <td>0.916625</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.563622</td>\n",
       "      <td>0.572372</td>\n",
       "      <td>0.567573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[179, 16, 7]</td>\n",
       "      <td>[relu, softmax]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.600814</td>\n",
       "      <td>1.657420</td>\n",
       "      <td>0.292250</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.041920</td>\n",
       "      <td>0.142614</td>\n",
       "      <td>0.064794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>[179, 16, 7]</td>\n",
       "      <td>[tanh, softmax]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.401251</td>\n",
       "      <td>0.729021</td>\n",
       "      <td>0.852625</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.646522</td>\n",
       "      <td>0.622865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>[179, 16, 8, 7]</td>\n",
       "      <td>[relu, relu, softmax]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.346954</td>\n",
       "      <td>0.683483</td>\n",
       "      <td>0.878625</td>\n",
       "      <td>0.7810</td>\n",
       "      <td>0.7810</td>\n",
       "      <td>0.604245</td>\n",
       "      <td>0.537451</td>\n",
       "      <td>0.555477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>[179, 16, 8, 7]</td>\n",
       "      <td>[tanh, relu, softmax]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.251210</td>\n",
       "      <td>0.744486</td>\n",
       "      <td>0.902875</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.574360</td>\n",
       "      <td>0.607085</td>\n",
       "      <td>0.577096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>[179, 16, 8, 7]</td>\n",
       "      <td>[tanh, tanh, softmax]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.384089</td>\n",
       "      <td>0.845680</td>\n",
       "      <td>0.849125</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.6920</td>\n",
       "      <td>0.463394</td>\n",
       "      <td>0.490017</td>\n",
       "      <td>0.474423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>[179, 16, 8, 7]</td>\n",
       "      <td>[relu, relu, softmax]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.602039</td>\n",
       "      <td>1.608216</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.2935</td>\n",
       "      <td>0.2935</td>\n",
       "      <td>0.041929</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.064830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>[179, 16, 8, 7]</td>\n",
       "      <td>[tanh, relu, softmax]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.212618</td>\n",
       "      <td>1.586251</td>\n",
       "      <td>0.542375</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>0.282994</td>\n",
       "      <td>0.261793</td>\n",
       "      <td>0.229463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>[179, 16, 8, 7]</td>\n",
       "      <td>[tanh, tanh, softmax]</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.667426</td>\n",
       "      <td>0.709043</td>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.7215</td>\n",
       "      <td>0.7215</td>\n",
       "      <td>0.492891</td>\n",
       "      <td>0.515399</td>\n",
       "      <td>0.501629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    config_id           layers            activations  learning_rate  \\\n",
       "0           1         [179, 7]              [softmax]           0.01   \n",
       "1           2         [179, 7]              [softmax]           0.10   \n",
       "2           3     [179, 16, 7]        [relu, softmax]           0.01   \n",
       "3           4     [179, 16, 7]        [tanh, softmax]           0.01   \n",
       "4           5     [179, 16, 7]        [relu, softmax]           0.10   \n",
       "5           6     [179, 16, 7]        [tanh, softmax]           0.10   \n",
       "6           7  [179, 16, 8, 7]  [relu, relu, softmax]           0.01   \n",
       "7           8  [179, 16, 8, 7]  [tanh, relu, softmax]           0.01   \n",
       "8           9  [179, 16, 8, 7]  [tanh, tanh, softmax]           0.01   \n",
       "9          10  [179, 16, 8, 7]  [relu, relu, softmax]           0.10   \n",
       "10         11  [179, 16, 8, 7]  [tanh, relu, softmax]           0.10   \n",
       "11         12  [179, 16, 8, 7]  [tanh, tanh, softmax]           0.10   \n",
       "\n",
       "    train_loss  val_loss  train_acc  val_acc  accuracy  precision    recall  \\\n",
       "0     0.270317  0.427863   0.892375   0.8405    0.8405   0.714092  0.706593   \n",
       "1     1.057552  1.660113   0.852125   0.8250    0.8250   0.700871  0.710071   \n",
       "2     0.269263  0.601914   0.903375   0.8520    0.8520   0.716525  0.746271   \n",
       "3     0.227352  0.700322   0.916625   0.7710    0.7710   0.563622  0.572372   \n",
       "4     1.600814  1.657420   0.292250   0.2930    0.2930   0.041920  0.142614   \n",
       "5     0.401251  0.729021   0.852625   0.7900    0.7900   0.623656  0.646522   \n",
       "6     0.346954  0.683483   0.878625   0.7810    0.7810   0.604245  0.537451   \n",
       "7     0.251210  0.744486   0.902875   0.7820    0.7820   0.574360  0.607085   \n",
       "8     0.384089  0.845680   0.849125   0.6920    0.6920   0.463394  0.490017   \n",
       "9     1.602039  1.608216   0.292000   0.2935    0.2935   0.041929  0.142857   \n",
       "10    1.212618  1.586251   0.542375   0.5065    0.5065   0.282994  0.261793   \n",
       "11    0.667426  0.709043   0.729750   0.7215    0.7215   0.492891  0.515399   \n",
       "\n",
       "    f1_score  \n",
       "0   0.707552  \n",
       "1   0.700474  \n",
       "2   0.701078  \n",
       "3   0.567573  \n",
       "4   0.064794  \n",
       "5   0.622865  \n",
       "6   0.555477  \n",
       "7   0.577096  \n",
       "8   0.474423  \n",
       "9   0.064830  \n",
       "10  0.229463  \n",
       "11  0.501629  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
